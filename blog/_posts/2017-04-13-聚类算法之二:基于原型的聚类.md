---
layout: post
title:  "聚类算法之二:基于原型的聚类"
date:   2017-04-13
---
<br>最近在关注聚类分析,了解了之后才发现,原来聚类分析里已经有这么丰富的成果,因此希望对其做个较全面的总结.
<br>本文会涉及的聚类算法有:
* [层次(系统)聚类(Agglomerative Clustering)](https://liangyaorong.github.io/blog/2017/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E4%B8%80-%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/)
1. 凝聚层次聚类
2. 分裂层次聚类
* [基于原型的聚类](https://liangyaorong.github.io/blog/2017/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E4%BA%8C-%E5%9F%BA%E4%BA%8E%E5%8E%9F%E5%9E%8B%E7%9A%84%E8%81%9A%E7%B1%BB/)
1. K-均值(K-means)
2. 二分K-均值(bisecting K-means)
3. K-中心(K-mediods)
4. 模糊C均值聚类(FCM)
4. SOM
* [基于密度的聚类](https://liangyaorong.github.io/blog/2017/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E4%B8%89-%E5%9F%BA%E4%BA%8E%E5%AF%86%E5%BA%A6%E7%9A%84%E8%81%9A%E7%B1%BB/)
1. DBSCAN
2. 基于网格的聚类(CLIQUE)
3. DENCLUE
* [基于图的聚类](https://liangyaorong.github.io/blog/2017/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E5%9B%9B-%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%9A%84%E8%81%9A%E7%B1%BB/)
1. 最小生成树聚类(MST)
2. OPOSSUM
3. Chameleon
4. SNN

## 基于原型的聚类
基于原型的定义是每个对象到该簇的原型的距离比到其他簇的原型的距离更近。其中,原型指样本空间中具有代表性的点.
<br>通俗地讲，就是对象离哪个簇近，这个对象就属于哪个簇。因此这种聚类的核心是如何确定簇的原型.
<br>
### K-均值(K-means)
取原型为样本均值.即样本质心.K-means中的K指簇的个数.
<br>目标函数函数为:
<br>![](http://latex.codecogs.com/gif.latex?J=\sum_{i=1}^{n}\sum_{j=1}^{c} U_{i,j}d(x_{i},c_{j}))
<br>其中,
<br>![](http://latex.codecogs.com/gif.latex?U_{i,j}=
\left\{\begin{matrix}
1 & \forall k\neq j,d(x_{i},c_{j})\leqslant d(x_{i},c_{k})\\ 
0 & others
\end{matrix}\right.)
<br>可以认为,
![](http://latex.codecogs.com/gif.latex?U_{i,j})是![](http://latex.codecogs.com/gif.latex?x_{i})所属簇的特征函数;
也可认为是样本![](http://latex.codecogs.com/gif.latex?x_{i})隶属于簇的隶属度.隶属为1,不隶属为0.
<br>
<br>其算法流程如下:
<br>![](http://img.blog.csdn.net/20170417122226183)
<br>
<br>下图展示了对n个样本点进行K-Means聚类的效果，这里K取2。
<br>![](http://img.blog.csdn.net/20170417123253063)
<br>
<br>**K-means的优缺点**
* 优点:
1. 计算速度快(算法复杂度![](http://latex.codecogs.com/gif.latex?O(mk*round))),原理简单.
* 缺点:
1. K难以确定.
2. 受初始质心影响较大.
3. 对异常值非常敏感(平均确定质心).

### 二分K-均值(bisecting K-means)
为克服 K-均值 算法收敛于局部最小值的问题,有人提出了另一个称为 二分K-均值 的算法.该算法首先将所有点作为一个簇,然后利用 K-means(K=2) 将该簇一分为二。之后选择其中一个簇继续进行划分,选择哪一个簇进行划分取决于对其划分是否可以最大程度降低总SSE的值。上述基于SSE的划分过程不断重复,直到得到用户指定的簇数目为止。
<br>
<br>流程图如下:
<br>![](http://img.blog.csdn.net/20170417191205595)
<br>
### K-中心(K-mediods)
从K-means中我们可以看到,它对异常点非常敏感.造成这个缺点的原因在于,每轮更新质点的时候是取簇中样本的平均.
<br>要解决这个问题可以改变质点的更新方法.
<br>在 K-medoids中，我们将从当前簇中选取这样一个点作为中心点,使它到簇中其他所有点的距离之和最小。
<br>其他步骤和K-means一致.
<br>
<br>**K-mediods的优缺点**
* 优点:
1. 解决K-means对异常点敏感的问题
* 缺点:
1. 由于要对每个簇中样本点进行遍历来寻找中心点,因此计算复杂度![](http://latex.codecogs.com/gif.latex?O((mk+m)*round))较K-means大.因此只适用于较小的样本.

### 模糊C均值(FCM)
上面提到的K-均值聚类,其实它有另一个名字,C-均值聚类(HCM).要讲模糊C-均值,我们先从C-均值,也就是K-均值这个角度谈起.
<br>我们已经知道了,K-均值的目标函数是






