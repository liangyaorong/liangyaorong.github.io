---
layout: post
title:  "聚类算法之二:基于原型的聚类"
date:   2017-04-13
---
<br>最近在关注聚类分析,了解了之后才发现,原来聚类分析里已经有这么丰富的成果,因此希望对其做个较全面的总结.
<br>本文会涉及的聚类算法有:
* [层次(系统)聚类(Agglomerative Clustering)](https://liangyaorong.github.io/blog/2017/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E4%B8%80-%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/)
1. 凝聚层次聚类
2. 分裂层次聚类
* [基于原型的聚类](https://liangyaorong.github.io/blog/2017/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E4%BA%8C-%E5%9F%BA%E4%BA%8E%E5%8E%9F%E5%9E%8B%E7%9A%84%E8%81%9A%E7%B1%BB/)
1. K-均值(K-means)
2. 二分K-均值(bisecting K-means)
3. K-中心(K-mediods)
4. 模糊C均值聚类(FCM)
4. SOM
* [基于密度的聚类](https://liangyaorong.github.io/blog/2017/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E4%B8%89-%E5%9F%BA%E4%BA%8E%E5%AF%86%E5%BA%A6%E7%9A%84%E8%81%9A%E7%B1%BB/)
1. DBSCAN
2. 基于网格的聚类(CLIQUE)
3. DENCLUE
* [基于图的聚类](https://liangyaorong.github.io/blog/2017/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E5%9B%9B-%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%9A%84%E8%81%9A%E7%B1%BB/)
1. 最小生成树聚类(MST)
2. OPOSSUM
3. Chameleon
4. SNN

## 基于原型的聚类
基于原型的定义是每个对象到该簇的原型的距离比到其他簇的原型的距离更近。其中,原型指样本空间中具有代表性的点.
<br>通俗地讲，就是对象离哪个簇近，这个对象就属于哪个簇。因此这种聚类的核心是如何确定簇的原型.
<br>
### K-均值(K-means)
* 取原型为样本均值.即样本质心.K-means中的K指簇的个数.
<br>目标函数函数为:
<br>![](http://latex.codecogs.com/gif.latex?J=\sum_{i=1}^{n}\sum_{j=1}^{c} U_{i,j}d(x_{i},c_{j}))
<br>其中,
<br>![](http://latex.codecogs.com/gif.latex?U_{i,j}=
\left\{\begin{matrix}
1 & \forall k\neq j,d(x_{i},c_{j})\leqslant d(x_{i},c_{k})\\ 
0 & others
\end{matrix}\right.)
<br>可以认为,
![](http://latex.codecogs.com/gif.latex?U_{i,j})是
![](http://latex.codecogs.com/gif.latex?x_{i})所属簇的特征函数;
也可认为是样本
![](http://latex.codecogs.com/gif.latex?x_{i})隶属于簇的隶属度.隶属为1,不隶属为0.
<br>
<br>其算法流程如下:
<br>![](http://img.blog.csdn.net/20170417122226183)
<br>
<br>下图展示了对n个样本点进行K-Means聚类的效果，这里K取2。
<br>![](http://img.blog.csdn.net/20170417123253063)
<br>
* K-means的优缺点
<br>优点:
<br>1. 计算速度快(算法复杂度![](http://latex.codecogs.com/gif.latex?O(mk\cdot round))),原理简单.
<br>缺点:
<br>1. K难以确定.
<br>2. 受初始质心影响较大.
<br>3. 对异常值非常敏感(平均确定质心).

### 二分K-均值(bisecting K-means)
* 为克服 K-均值 算法收敛于局部最小值的问题,有人提出了另一个称为 二分K-均值 的算法.该算法首先将所有点作为一个簇,然后利用 K-means(K=2) 将该簇一分为二。之后选择其中一个簇继续进行划分,选择哪一个簇进行划分取决于对其划分是否可以最大程度降低总SSE的值。上述基于SSE的划分过程不断重复,直到得到用户指定的簇数目为止。
<br>流程图如下:
<br>![](http://img.blog.csdn.net/20170417191205595)
<br>
### K-中心(K-mediods)
* 从K-means中我们可以看到,它对异常点非常敏感.造成这个缺点的原因在于,每轮更新质点的时候是取簇中样本的平均.
<br>要解决这个问题可以改变质点的更新方法.
<br>在 K-medoids中，我们将从当前簇中选取这样一个点作为中心点,使它到簇中其他所有点的距离之和最小。
<br>其他步骤和K-means一致.
<br>
* K-mediods的优缺点
<br>优点:
<br>1. 解决K-means对异常点敏感的问题
<br>缺点:
<br>1. 由于要对每个簇中样本点进行遍历来寻找中心点,因此计算复杂度![](http://latex.codecogs.com/gif.latex?O((mk+m)\cdot round))较K-means大.因此只适用于较小的样本.

### 模糊C均值(FCM)
上面提到的K-均值聚类,其实它有另一个名字,C-均值聚类(HCM).要讲模糊C-均值,我们先从C-均值,也就是K-均值这个角度谈起.
<br>
* HCM
<br>我们已经知道了,K-均值的目标函数是
<br>![](http://latex.codecogs.com/gif.latex?J=\sum_{i=1}^{n}\sum_{j=1}^{c} u_{i,j}d(x_{i},c_{j}))
<br>其中,
<br>![](http://latex.codecogs.com/gif.latex?u_{ij}=
\left\{\begin{matrix}
1 & \forall k\neq j,d(x_{i},c_{j})\leqslant d(x_{i},c_{k})\\ 
0 & others
\end{matrix}\right.)
<br>这也是为什么叫H(Hard)CM.要么隶属该簇,要么不隶属该簇.
<br>引入模糊数学的观点,使得每个给定数据点用值在0，1间的隶属度来确定其属于各个组的程度,便得到FCM的核心思想.
<br>
* FCM
<br>FCM的目标函数是
<br>![](http://latex.codecogs.com/gif.latex?J=\sum_{i=1}^{n}\sum_{j=1}^{c} u_{ij}^{m}d(x_{i},c_{j}))
<br>其中
<br>![](http://latex.codecogs.com/gif.latex?u_{ij})介于0,1之间;m为是一个控制算法的柔性的参数.
<br>而且对任意样本![](http://latex.codecogs.com/gif.latex?x_{i})满足:
<br>![](http://latex.codecogs.com/gif.latex?\sum_{j=1}^{c} u_{ij}=1)
<br>
<br>借助拉格朗日乘子,我们将限制条件整合到目标函数中得到新的目标函数
<br>
<br>![](http://latex.codecogs.com/gif.latex?J'=\sum_{i=1}^{n}\sum_{j=1}^{c} (u_{ij})^{m}d(x_{i},c_{j})+\sum_{i=1}^{n}\lambda_{i}(\sum_{j=1}^{c}u_{ij}-1))
<br>
<br>即在每一轮迭代中,寻找![](http://latex.codecogs.com/gif.latex?u_{ij})使得目标函数最小.
<br>
<br>下面我们对其进行求解:
1. 当![](http://latex.codecogs.com/gif.latex?d_{ij})为欧氏距离,对中心![](http://latex.codecogs.com/gif.latex?c_{i})偏导,有
<br>
<br>![](http://latex.codecogs.com/gif.latex?\frac{\partial J'}{\partial c_{j}}=\sum_{i=1}^{n}(u_{ij})^{m}x_{i}-c_{j}\sum_{j=1}^{n}(u_{ij})^{m}=0 \ \ \ \ \ (1))
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow c_{j}=\frac{\sum_{i=1}^{n}(u_{ij})^{m}x_{i}}{\sum_{i=1}^{n}(u_{ij})^{m}}\ \ \ \ \ (2)) 
<br>
2. ![](http://latex.codecogs.com/gif.latex?J')对变量![](http://latex.codecogs.com/gif.latex?u_{ij})求偏导并令其为0,得到:
<br>![](http://latex.codecogs.com/gif.latex?\frac{\partial J'}{\partial u_{ij}}=m(u_{ij})^{m-1}\cdot d(x_{i},c_{j})+\lambda_{j} =0 \ \ \ \ \ (3))
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow u_{ij}=\bigl(\begin{smallmatrix}
\frac{-\lambda_{j}}{m\cdot d(x_{i},c_{j})}
\end{smallmatrix}\bigr)^{\frac{1}{m-1}}\ \ \ \ \ (4))
<br>
3. 为了使公式(4)满足条件![](http://latex.codecogs.com/gif.latex?\sum_{j=1}^{c} u_{ij}=1),我们将公式(4)代入条件,得到:
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow \sum_{j=1}^{c}u_{ij}=\sum_{j=1}^{c}\bigl(\begin{smallmatrix}
\frac{(-\lambda_{j})^{\frac{1}{m-1}}}{[m\cdot d(x_{i},c_{j})]^{\frac{1}{m-1}}}
\end{smallmatrix}\bigr)=1)
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  (-\lambda_{j})^{\frac{1}{m-1}} \sum_{j=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{j}) }
\end{bmatrix}^{\frac{1}{m-1}}=1)
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  (-\lambda_{j})^{\frac{1}{m-1}}= \begin{Bmatrix}
\sum_{j=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{j}) }
\end{bmatrix}^{\frac{1}{m-1}}
\end{Bmatrix}^{-1}\ \ \ \ \ (5))
4. 看,我们的到了公式(5),即![](http://latex.codecogs.com/gif.latex?\lambda_{j})所需满足的条件.将公式(5)代回公式(4),我们得到:
<br>![](http://latex.codecogs.com/gif.latex?u_{ij}= \frac{\begin{Bmatrix}
\sum_{k=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{k}) }
\end{bmatrix}^{\frac{1}{m-1}}
\end{Bmatrix}^{-1}}{\begin{bmatrix}
m\cdot d(x_{i},c_{j})
\end{bmatrix}^{\frac{1}{m-1}}})
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\begin{bmatrix}
m\cdot d(x_{i},c_{j})
\end{bmatrix}^{\frac{1}{m-1}}
\cdot 
\begin{Bmatrix}
\sum_{k=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{k}) }
\end{bmatrix}^{\frac{1}{m-1}}
\end{Bmatrix}})
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\sum_{k=1}^{c}\begin{bmatrix}
\frac{m\cdot d(x_{i},c_{j})}{m\cdot d(x_{i},c_{k})}
\end{bmatrix}^{\frac{1}{m-1}}})
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\sum_{k=1}^{c}\begin{bmatrix}
\frac{d(x_{i},c_{j})}{d(x_{i},c_{k})}
\end{bmatrix}^{\frac{1}{m-1}}}\ \ \ \ \ (6))
<br>
<br>数学推导到这里就结束了,我们关键再看一下公式(2)与公式(6).这两个公式分别决定了簇中心与隶属矩阵的更新.
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow c_{j}=\frac{\sum_{i=1}^{n}(u_{ij})^{m}x_{i}}{\sum_{i=1}^{n}(u_{ij})^{m}}\ \ \ \ \ (2)) 
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\sum_{k=1}^{c}\begin{bmatrix}
\frac{d(x_{i},c_{j})}{d(x_{i},c_{k})}
\end{bmatrix}^{\frac{1}{m-1}}}\ \ \ \ \ (6))
<br>
* 模糊C均值的算法流程如下:










