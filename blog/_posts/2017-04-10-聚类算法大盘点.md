---
layout: post
title:  "聚类算法大盘点"
date:   2017-04-10
---
<br>最近在关注聚类分析,了解了之后才发现,原来聚类分析里已经有这么丰富的成果,因此希望对其做个较全面的总结.
<br>本文涉及到的聚类算法较多,请允许我慢慢更新.
<br>
* [1 层次(系统)聚类(Agglomerative Clustering)](#1)
<br>[1.1 凝聚层次聚类](#1.1)
<br>[1.2 分裂层次聚类](#1.2)
* [2 基于原型的聚类](#2)
<br>[2.1 K-均值(K-means)](#2.1)
<br>[2.2 二分K-均值(bisecting K-means)](#2.2)
<br>[2.3 K-中心(K-mediods)](#2.3)
<br>[2.4 模糊C均值聚类(FCM)](#2.4)
* [3 基于密度的聚类](#3)
<br>[3.1 DBSCAN](#3.1)
<br>[3.2 OPTICS](#3.2)
<br>[3.3 DENCLUE](#3.3)
* [4 基于网格的聚类](#4)
<br>[4.1 STING](#4.1)
<br>[4.2 CLIQUE](#4.2)
* [5 基于图的聚类](#5)
<br>[5.1 最小生成树聚类(MST)](#5.1)
<br>[5.2 OPOSSUM](#5.2)
<br>[5.3 Chameleon](#5.3)
<br>[5.4 SNN](#5.4)
* [6 基于神经网络](#6)
<br>[6.1 SOM](#6.1)

<h3 id="1">1 层次聚类 (Agglomerative Clustering)</h3>
层次聚类也叫系统聚类,和K-means一起是最常用的聚类方式.
<br>聚类效果如下:
<br>![](http://img.blog.csdn.net/20170413164051502)
<br>它的实现方法有两种:
1. 凝聚法
<br>自下而上.从点作为个体簇开始.迭代时每一步合并两个最接近的簇,直到所有样本合并为一簇.
2. 分裂法
<br>自上而下.从包含所有样本点的某个簇开始.迭代时每一步分裂一个个体簇,直到所有簇都为个体簇.

<h4 id="1.1">1.1 凝聚法</h4>
凝聚法的关键是合并两个最接近的簇.合并的依据就是**簇间的距离**.不同的距离通常会得到不同的聚类结果.常用的有如下几种:
1. 簇间最小距离
<br>![](http://img.blog.csdn.net/20170415225837726)
2. 簇间最大距离
<br>![](http://img.blog.csdn.net/20170415225809085)
3. 簇间平均距离
<br>![](http://img.blog.csdn.net/20170415231046834)
4. 簇间质心距离
<br>![](http://img.blog.csdn.net/20170415231438555)
5. Ward方法
<br>两个簇合的临近度定义为**两个簇合并时导致的平方误离差平方和的增量**.每次选取增量最小的两簇进行合并.该思想来源于方差分析:若分类正确,同类样平的离差平方和应该小,类与类之间的离差平方和应该大

我们可以看到,除Ward方法之外,簇间距离也依赖于不同簇中的点间距离.
<br>点间距离可以采用欧氏距离,马氏距离,闵氏距离等等.
<br>更多的点间距离可以看这篇文章 [传送门](https://liangyaorong.github.io/blog/2016/%E5%B8%B8%E7%94%A8%E8%B7%9D%E7%A6%BB%E6%80%BB%E7%BB%93/)

<h4 id="1.2">1.2 分裂法</h4>
分裂法与凝聚法刚好相反.每次分裂出一个个体簇.主要的实现方式是"最小生成树聚类".在这里先放着不讲.

* 层次聚类的优缺点
<br>优点:
<br>1. 聚类质量较高.
<br>缺点:
<br>1. 算法复杂度![](http://latex.codecogs.com/gif.latex?O(m^{2}logm)),空间复杂度![](http://latex.codecogs.com/gif.latex?O(m^{2})).这都相当昂贵.不利于其大规模运用.
<br>2. 缺乏全局目标函数.

<h3 id="2">2 基于原型的聚类 (Agglomerative Clustering)</h3>
基于原型的定义是每个对象到该簇的原型的距离比到其他簇的原型的距离更近。其中,原型指样本空间中具有代表性的点.
<br>通俗地讲，就是对象离哪个簇近，这个对象就属于哪个簇。因此这种聚类的核心是如何确定簇的原型.

<h4 id="2.1">2.1 K-均值 (K-means)</h4>
* 取原型为样本均值.即样本质心.K-means中的K指簇的个数.
<br>目标函数函数为:
<br>![](http://latex.codecogs.com/gif.latex?J=\sum_{i=1}^{n}\sum_{j=1}^{c} U_{i,j}d(x_{i},c_{j}))
<br>其中,
<br>![](http://latex.codecogs.com/gif.latex?U_{i,j}=
\left\{\begin{matrix}
1 & \forall k\neq j,d(x_{i},c_{j})\leqslant d(x_{i},c_{k})\\ 
0 & others
\end{matrix}\right.)
<br>可以认为,
![](http://latex.codecogs.com/gif.latex?U_{i,j})是
![](http://latex.codecogs.com/gif.latex?x_{i})所属簇的特征函数;
也可认为是样本
![](http://latex.codecogs.com/gif.latex?x_{i})隶属于簇的隶属度.隶属为1,不隶属为0.
<br>
* 算法流程如下:
<br>![](http://img.blog.csdn.net/20170417122226183)
<br>
<br>下图展示了对n个样本点进行K-Means聚类的效果，这里K取2。
<br>![](http://img.blog.csdn.net/20170417123253063)
<br>
* K-means的优缺点
<br>优点:
<br>1. 计算速度快(算法复杂度![](http://latex.codecogs.com/gif.latex?O(mk\cdot round))),原理简单.
<br>缺点:
<br>1. K难以确定.
<br>2. 受初始质心影响较大.
<br>3. 对异常值非常敏感(平均确定质心).

<h4 id="2.2">2.2 二分K-均值 (bisecting K-means)</h4>
* 为克服 K-均值 算法收敛于局部最小值的问题,有人提出了另一个称为 二分K-均值 的算法.该算法首先将所有点作为一个簇,然后利用 K-means(K=2) 将该簇一分为二。之后选择其中一个簇继续进行划分,选择哪一个簇进行划分取决于对其划分是否可以最大程度降低总SSE的值。上述基于SSE的划分过程不断重复,直到得到用户指定的簇数目为止。
* 算法流程如下:
<br>![](http://img.blog.csdn.net/20170417191205595)

<h4 id="2.3">2.3 K-中心 (K-mediods)</h4>
* 从K-means中我们可以看到,它对异常点非常敏感.造成这个缺点的原因在于,每轮更新质点的时候是取簇中样本的平均.
<br>要解决这个问题可以改变质点的更新方法.
<br>在 K-medoids中，我们将从当前簇中选取这样一个点作为中心点,使它到簇中其他所有点的距离之和最小。
<br>其他步骤和K-means一致.
<br>
* K-mediods的优缺点
<br>优点:
<br>1. 解决K-means对异常点敏感的问题
<br>缺点:
<br>1. 由于要对每个簇中样本点进行遍历来寻找中心点,因此计算复杂度![](http://latex.codecogs.com/gif.latex?O((mk+m)\cdot round))较K-means大.因此只适用于较小的样本.

<h4 id="2.4">2.4 模糊C均值 (FCM)</h4>
上面提到的K-均值聚类,其实它有另一个名字,C-均值聚类(HCM).要讲模糊C-均值,我们先从C-均值,也就是K-均值这个角度谈起.
<br>
* HCM
<br>从上面对K-均值算法的介绍中,我们已经知道了,K-均值的目标函数是:
<br>![](http://latex.codecogs.com/gif.latex?J=\sum_{i=1}^{n}\sum_{j=1}^{c} u_{ij}d(x_{i},c_{j}))
<br>其中,
<br>![](http://latex.codecogs.com/gif.latex?u_{ij}=
\left\{\begin{matrix}
1 & \forall k\neq j,d(x_{i},c_{j})\leqslant d(x_{i},c_{k})\\ 
0 & others
\end{matrix}\right.)
<br>
<br>这也是为什么叫H(Hard)CM.要么隶属该簇,要么不隶属该簇.太硬了.
<br>引入模糊数学的观点,使得每个给定数据点用值在0，1间的隶度来确定其属于各个组的程度,便得到FCM的核心思想.
<br>
* FCM
<br>FCM的目标函数是
<br>![](http://latex.codecogs.com/gif.latex?J=\sum_{i=1}^{n}\sum_{j=1}^{c} u_{ij}^{m}d(x_{i},c_{j}))
<br>其中
<br>![](http://latex.codecogs.com/gif.latex?u_{ij})介于0,1之间;
<br>![](http://latex.codecogs.com/gif.latex?m\in [1,+\infty ))为控制算法模糊程度的参数.
<br>而且,对任意样本![](http://latex.codecogs.com/gif.latex?x_{i})满足:
<br>![](http://latex.codecogs.com/gif.latex?\sum_{j=1}^{c} u_{ij}=1)
<br>
<br>借助拉格朗日乘子,我们将限制条件整合到目标函数中得到新的目标函数
<br>
<br>![](http://latex.codecogs.com/gif.latex?J'=\sum_{i=1}^{n}\sum_{j=1}^{c} (u_{ij})^{m}d(x_{i},c_{j})+\sum_{i=1}^{n}\lambda_{i}(\sum_{j=1}^{c}u_{ij}-1))
<br>
<br>即在每一轮迭代中,要寻找![](http://latex.codecogs.com/gif.latex?u_{ij})使得目标函数最小.
<br>
<br>下面我们对其进行求解:
1. 当![](http://latex.codecogs.com/gif.latex?d_{ij})为欧氏距离,![](http://latex.codecogs.com/gif.latex?J')对中心![](http://latex.codecogs.com/gif.latex?c_{i})求偏导并令其为0,有
<br>
<br>![](http://latex.codecogs.com/gif.latex?\frac{\partial J'}{\partial c_{j}}=\sum_{i=1}^{n}(u_{ij})^{m}x_{i}-c_{j}\sum_{j=1}^{n}(u_{ij})^{m}=0 \ \ \ \ \ (1))
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow c_{j}=\frac{\sum_{i=1}^{n}(u_{ij})^{m}x_{i}}{\sum_{i=1}^{n}(u_{ij})^{m}}\ \ \ \ \ (2)) 
<br>
2. ![](http://latex.codecogs.com/gif.latex?J')对变量![](http://latex.codecogs.com/gif.latex?u_{ij})求偏导并令其为0,有:
<br>![](http://latex.codecogs.com/gif.latex?\frac{\partial J'}{\partial u_{ij}}=m(u_{ij})^{m-1}\cdot d(x_{i},c_{j})+\lambda_{j} =0 \ \ \ \ \ (3))
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow u_{ij}=\bigl(\begin{smallmatrix}
\frac{-\lambda_{j}}{m\cdot d(x_{i},c_{j})}
\end{smallmatrix}\bigr)^{\frac{1}{m-1}}\ \ \ \ \ (4))
<br>
<br>为了使公式(4)满足条件![](http://latex.codecogs.com/gif.latex?\sum_{j=1}^{c} u_{ij}=1),我们将公式(4)代入条件,得到:
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow \sum_{j=1}^{c}u_{ij}=\sum_{j=1}^{c}\bigl(\begin{smallmatrix}
\frac{(-\lambda_{j})^{\frac{1}{m-1}}}{[m\cdot d(x_{i},c_{j})]^{\frac{1}{m-1}}}
\end{smallmatrix}\bigr)=1)
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  (-\lambda_{j})^{\frac{1}{m-1}} \sum_{j=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{j}) }
\end{bmatrix}^{\frac{1}{m-1}}=1)
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  (-\lambda_{j})^{\frac{1}{m-1}}= \begin{Bmatrix}
\sum_{j=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{j}) }
\end{bmatrix}^{\frac{1}{m-1}}
\end{Bmatrix}^{-1}\ \ \ \ \ (5))
<br>
<br>看,我们得到了公式(5),即![](http://latex.codecogs.com/gif.latex?\lambda_{j})所需满足的条件.接下来将公式(5)代回到公式(4),我们得到:
<br>![](http://latex.codecogs.com/gif.latex?u_{ij}= \frac{\begin{Bmatrix}
\sum_{k=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{k}) }
\end{bmatrix}^{\frac{1}{m-1}}
\end{Bmatrix}^{-1}}{\begin{bmatrix}
m\cdot d(x_{i},c_{j})
\end{bmatrix}^{\frac{1}{m-1}}})
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\begin{bmatrix}
m\cdot d(x_{i},c_{j})
\end{bmatrix}^{\frac{1}{m-1}}
\cdot 
\begin{Bmatrix}
\sum_{k=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{k}) }
\end{bmatrix}^{\frac{1}{m-1}}
\end{Bmatrix}})
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\sum_{k=1}^{c}\begin{bmatrix}
\frac{m\cdot d(x_{i},c_{j})}{m\cdot d(x_{i},c_{k})}
\end{bmatrix}^{\frac{1}{m-1}}})
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\sum_{k=1}^{c}\begin{bmatrix}
\frac{d(x_{i},c_{j})}{d(x_{i},c_{k})}
\end{bmatrix}^{\frac{1}{m-1}}}\ \ \ \ \ (6))
<br>
3. 数学推导到这里就结束了,我们关键再看一下公式(2)与公式(6).这两个公式分别决定了簇中心与隶属矩阵的更新.
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow c_{j}=\frac{\sum_{i=1}^{n}(u_{ij})^{m}x_{i}}{\sum_{i=1}^{n}(u_{ij})^{m}}\ \ \ \ \ (2)) 
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\sum_{k=1}^{c}\begin{bmatrix}
\frac{d(x_{i},c_{j})}{d(x_{i},c_{k})}
\end{bmatrix}^{\frac{1}{m-1}}}\ \ \ \ \ (6))
<br>
* 模糊C均值的算法流程如下:
<br>![](http://img.blog.csdn.net/20170418202647364)
<br>
<br>当算法收敛,就可以得到聚类中心和各个样本对于各簇的隶属度值,从而完成了模糊聚类划分.如果有需要,还可以将模糊聚类结果进行去模糊化.即用一定规则把模糊聚类划分为确定性分类.

<h3 id="3">3 基于密度的聚类</h3>
基于密度的聚类寻找被低密度区域分离的高密度区域.
<br>对密度的不同定义衍生出不同的聚类方式
<br>
<h4 id="3.1">3.1 DBSCAN</h4>
 DBSCAN,全称为 Density-Based Spatial Clustering of Applications with Noise.从名字可以看出,它能识别噪声.当然,噪声不属于任何一类.
<br>DBSCAN用基于中心的方法定义密度.基于中心,指根据中心邻域内样点的数量衡量密度.
* DBSCAN的预备知识.
1. 核心点(core point): 给定邻域半径Eps(![](http://latex.codecogs.com/gif.latex?\epsilon )),样本点个数MinPts.若该点的Eps邻域内包含的样本点数>MinPts,则认为该点为核心点.
2. 边界点(border point): 该点不是核心点,但其在某个核心点的Eps邻域内.
3. 噪声点(noise point): 该点既非核心点,也非边界点.
4. 直接密度可达:若p在q的Eps邻域内,且q为核心点,则认为,p从q出发是直接密度可达的.
5. 密度可达: 若存在某一样本链![](http://latex.codecogs.com/gif.latex?p_{1},p_{2},p_{3},...,p_{n}).![](http://latex.codecogs.com/gif.latex?p_{i+1},i=1,2,...,n-1)从![](http://latex.codecogs.com/gif.latex?p_{i})出发是直接密度可达的,则认为![](http://latex.codecogs.com/gif.latex?p_{n})从![](http://latex.codecogs.com/gif.latex?p_{1})密度可达.
6. 密度相连: 若p,q都是从某点O密度可达的,则认为p,q密度相连.

* DBSCAN的核心思想:
<br>寻找最大的密度相连集合,并标记其为同一类.Eps(邻域半径)与Minpts(最少点数)由经验确定.
<br>
<br>![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/400px-DBSCAN-Illustration.svg.png)
<br>图中A为核心点; B,C为边界点；N为噪声点;
<br>B从A出发密度可达; C从A出发密度可达；A,B,C,红点密度相连.因此该图中{A,B,C,红点}为一簇
<br>
* DBSCAN的算法流程
<br>![](http://img.blog.csdn.net/20170419142508199)

* 聚类效果:
<br>![](http://img.blog.csdn.net/20170419144453053)

* DBSCAN算法优缺点:
<br>优点:
<br>1. 多次运行产生同样结果
<br>2. 自动确定簇的个数,不需设置初始化质心
<br>3. 可发现不同形状的簇
<br>缺点:
<br>1. 复杂度![](http://latex.codecogs.com/gif.latex?O(m^{2})),较K-means大.使用距离索引可降低复杂度到![](http://latex.codecogs.com/gif.latex?O(mlog(m))).
<br>2. 当簇具有不同密度时,表现不佳(只能设置一组Eps和Minpts)
<br>3. 与K-means相比,DBSCAN会合并重叠的簇;而K-means可以发现重叠的簇

<h4 id="3.2">3.2 OPTICS</h4>
讲DBSCAN时,我们说它有一个缺点:当簇具有不同密度时,表现不佳(只能设置一组Eps和Minpts).如下图所示:
![](http://img.blog.csdn.net/20170420173239820)
<br>图中显示了隐藏在噪声中的4个簇.簇和噪声的密度由明暗度表示.左方框内的噪声密度与簇C,簇D相等.这时候.若设置高密度为阈值,可以识别出簇A与簇B,但簇C与簇D会被认为是噪声.若设置低密度为阈值,可以识别出簇C与簇D,但左方框内所有样本点会被认为是同一簇(包括噪声)

* 为了解决DBSCAN这个弊端,有人提出了OPTICS(Ordering Points To Identify the CluStering)算法.
<br>
<br>为了更细致地刻画样本密度情况,我们在DBSCAN的基础上多定义两个概念.
1. 核心距离(core-distance):
<br>![](https://wikimedia.org/api/rest_v1/media/math/render/svg/355e66cb6c96df7c2105f0b12ca07da061d00813)
<br>即,给定MinPts,核心距离指使一个点成为核心点的最小邻域半径.
2. 可达距离(reachability-distance):
<br>![](https://wikimedia.org/api/rest_v1/media/math/render/svg/30f182c183cb28ee61717bfd95a67a469cd96682)
<br>即,从p直接密度可达o的最小邻域半径.(隐含前提是,p为核心点).显然,空间密度越大,p与相邻点的可达距离就越小

<br>
* OPTICS算法的核心思想:
1. 对样本点朝着**尽量稠密**的空间进行遍历,记录每次迭代中样本点的可达距离,得到可达图.
<br>![](http://img.blog.csdn.net/20170421131619642)
2. 由于我们是沿着尽量稠密的方向进行遍历,因此可达距离在逼近密度中心之前必然是递减的;而远离密度中心时必然是递增的.因此,可根据可达图判断出密度中心(极小值点),从而进行簇的划分.

<br>
* 算法流程
<br>![](http://img.blog.csdn.net/20170421133442479)

<br>
* 聚类效果:
<br>![](http://img.blog.csdn.net/20170421134635137)

<br>
* 算法优缺点:
<br>优点:
<br>可识别具有不同密度的簇.
<br>缺点:
<br>算法复杂度为![](http://latex.codecogs.com/gif.latex?O(m^{2}))


<h4 id="3.3">3.3 DENCLUE</h4>
DENCLUE (DENsity CLUstering). 它用与每个点相关联的影响函数之和对点集的总密度建模.结果总密度函数将具有局部尖峰(即局部密度最大值),以这些局部尖峰进行定簇.
<br>具体地说,每个尖峰为簇的中心.对于每个样本点,通过爬山过程,寻找其最近的尖峰,认为其与该尖峰相关联,并标记为对应簇.这与K-means有点类似.只不过K-means通过初始化随机中心并迭代得到最后的簇中心.而DENCLUE则根据核函数局部尖峰定义簇中心.
<br>同时,可以对核密度峰值加一个阈值限制.小于阈值的局部尖峰与其相关联的样本点都将认为是噪声,可以丢弃.
* 下面用图来作直观解释.
<br>![](http://img.blog.csdn.net/20170420002013034)
<br>如图所示,根据局部尖峰将这一维数据划分为A,B,C,D,E五簇(虚线为界)
<br>加上阈值![](http://latex.codecogs.com/gif.latex?\xi)后,簇C(虚线为界)认为是噪声点,可以舍弃.
<br>这里值得注意的是,与簇D相关联但小于阈值的样本点依然保留.类似于DBSCAN中的边界点.
<br>由于簇D与簇E相连接的山谷依然大于阈值,因此可以将簇D与簇E合并为一簇;而簇A与簇B之间相连接的山谷小于阈值,因此簇A与簇B依然各自为一簇.
<br>因此,添加阈值限制后可将数据集划分为三簇.

* 核密度估计(density estimation)
<br>从上面的过程我们也可以看到,怎样估计核密度成为该算法的核心.
<br>核密度估计这种技术的目标是通过函数描述数据的分布.对于核密度估计,每个点对总核密度的贡献用一个影响(influence)或核函数(kernel function)表示.总的核密度是与每个点相关联的影响函数之和.
<br>通常,核函数是对称的,而且它的值随到点的距离增加而减少.例如,高斯核函数就是一个典型例子.

<br>高斯核函数(对于某个点p):
<br>![](http://latex.codecogs.com/gif.latex?K(x,p)=e^{\frac{-distance(x,p)^{2}}{2\sigma ^{2}}})
<br>
<br>其实就是高斯分布的核.其中x为网格中某一点.
<br>该函数显示了样本点p对定义域中某点x的影响.
<br>
<br>因此,x的总核密度为:
<br>![](http://latex.codecogs.com/gif.latex?\sum_{i=1}^{n}K(x,p_{i})=\sum_{i=1}^{n}e^{\frac{-d(x,p_{i})^{2}}{2\sigma ^{2}}})
<br>
<br>下图给出了高斯核总密度函数的计算例子
<br>![](http://img.blog.csdn.net/20170420005808928)

* 算法流程
<br>![](http://img.blog.csdn.net/20170420010158992)

* 算法优缺点
<br>优点:
<br>1. 核密度的引入给出了密度的一个精确定义.较DBSCAN更具理论依据
<br>缺点:
<br>1. 密度的精确定义带来了算法复杂度的增加.定义域网格中每个点都要遍历所有样本点以计算总核密度.网格大小确定了计算开销:网格大,计算量小,精度降低;网格小,计算量大,精度提高.


<h3 id="4">4 基于网格的聚类</h3>
基于网格的聚类,其基本思想是:
1. 将每个属性的可能值分割成许多相邻的区间,创建网格单元的集合(类似于对各维特征进行等宽或等深分箱).将每个样本映射到网格单元中,同时统计每个网格单元的信息,如样本数,均值,中位数,最大最小值,分布等.之后所有处理的基本单位都是网格单元.
2. 删除密度低于某个指定阈值![](http://latex.codecogs.com/gif.latex?\tau)的网格单元.
3. 由稠密的单元组形成簇

ps:可以看到,基于网格的聚类或多或少与密度有关系

<br>举个二维例子
<br>![](http://img.blog.csdn.net/20170428002311274)
<br>若取阈值为8,则按单元格密度可划分为两类:
<br>![](http://img.blog.csdn.net/20170428002836898)
<br>

<h4 id="4.1">4.1 STING</h4>
STING (STatistical INformation Grid)算法本用于查询.但稍微修改一下便可用于聚类.
<br>下面先介绍STING查询算法.
<br>![](http://img.blog.csdn.net/20170428110854848)
<br>如图,我们将样本映射到不同分辨率的网格中.
<br>其中,高层的每个单元被划分为多个低一层的单元,如下图:
<br>![](http://img.blog.csdn.net/20170428110842738)
<br>
* STING查询算法流程
1. 首先，在层次结构中选定一层作为查询处理的开始层。
2. 对当前层次的每个单元，我们根据其统计信息，考察该单元与给定查询的关联程度。不相关的单元就不再考虑,低一层的处理就只检查剩余的相关单元。
3. 这个处理过程反复进行，直到达到最底层。此时，如果查询要求被满足，那么返回相关单元的区域。否则，检索和进一步的处理落在相关单元中的数据，直到它们满足查询要求.

这样查询的优点是每层的查询都可以过滤大量不相关的样本,从而缩小查询范围.
<br>
* STING聚类算法
将STING查询算法稍微修改一下就可以将其用于聚类.
<br>其流程如下:
1. 首先，在层次结构中选定一层作为聚类的开始层。
2. 对当前层次的每个单元，我们根据其统计信息(如样本数,样本数百分比等)与设定的阈值，考察该单元是否为噪声单元.噪声单元内所有点认为是噪声点,不参与聚类.低一层的处理就只检查剩余的非噪声单元。
3. 这个处理过程反复进行，直到达到最底层。最底层稠密的单元组形成簇.

<br>
* STING聚类算法优缺点
<br>优点:
<br>1. 效率高.顺序扫描一次样本即可获得所有网格统计信息,时间复杂度![](http://latex.codecogs.com/gif.latex?O(n)),n为样本数; 层次结构建立后,聚类时间是![](http://latex.codecogs.com/gif.latex?O(g)),g为层数.
<br>缺点:
<br>1. 非常依赖阈值选取.阈值太高,簇可能丢失; 阈值太低,本应分离的簇可能被合并,这也是基于网格的聚类通病.
<br>2. 聚类的质量取决于网格结构的最底层的粒度.如果粒度比较细，处理的代价会显著增加；但是，如果网格结构最底层的粒度太粗，将会降低聚类分析的质量.
<br>3. 聚类边界或者是水平的，或者是竖直的，没有斜的分界线.因此也不能识别圆形区域.这可能降低簇的质量和精确性.


<h4 id="4.2">4.2 CLIQUE</h4>
<br>CLIQUE (CLustering In QUEst)是综合运用基于密度和网格方法优点所构造的聚类方法.其核心思想是利用先验原理,找出在高维数据空间中存在的低维簇.在讲该算法之前,我们先了解一下"子空间聚类"
<br>
* 子空间聚类
<br>用特征的子集进行聚类.这可能会与特征全空间聚类的到的结果很不一样.有如下两个理由:
1. 数据中关于少数属性的集合可能可以聚类,而关于其他的属性是随机分布的.
2. 某些情况下,在不同的维集合中存在不同的簇.

举个例子:
<br>![](http://img.blog.csdn.net/20170428234126923)
<br>全空间中,样本被分成了三个簇,分别用"菱形","正方形","三角形".噪声用"圆形"标记.
<br>这个图解释了两个事实:
1. 圆点集在全空间中不形成簇,但在子空间中却可能成簇.
2. 存在于高维空间中的簇在低维空间中也将成簇(其投影成簇).

但是我们会发现,要寻找所有子空间并寻找其中的簇,子空间的数量是指数级的.这显然不现实的.需要剪枝来去掉不必要的寻找.CLIQUE就是为了完成这个工作.

* 先验原理
<br>从上面的第二个事实,我们可引出定理:如果一个网格单元在全特征空间中稠密,那在其子空间中也稠密.
<br>我们称之为先验原理(与Apriori算法中的先验原理一致)
<br>![](http://img.blog.csdn.net/20170429161119552)
<br>

* CLIQUE算法
<br>利用先验原理,我们可以减小需要判断的子空间范围.
<br>CLIQUE算法对子空间的簇的搜索,与Apriori算法中频繁项集的搜索一致.
<br>
<br>先贴出算法流程:
<br>![](http://img.blog.csdn.net/20170429000306791)
<br>
<br>用上面先验原理的图示来解释:
1. (一维子空间) a,b,c,d,e五维中 c,d,e三维根据网格密度判断,可以成簇,因此一维子空间候选集{c,d,e}
2. (二维子空间) 在一维子空间候选集的基础上进行元素组合,得到二维子空间集{cd,ce,de},判断集中每个子空间是否可以成簇.
<br>例如,若cd空间中网格单元密度均小于阈值,则cd子空间不可以成簇.我们将cd剔除,后续不再考虑以其为子空间的高维空间;同理若判断{ce,de}成簇,则二维子空间候选集为{ce,de}
3. (三维子空间) 在二维子空间候选集的基础上增加一维(该维来自一维子空间候选集),得到三维子空间集{cde}.若cde成簇,则得到三维子空间候选集{cde};若不成簇,因为不存在三维稠密单元,算法结束.

如果还有不清楚的建议看一下Apriori算法.
<br>

* 算法优缺点
<br>缺点:
<br>1. 与所有基于网格的聚类算法一致,阈值与网格较难确定.
<br>

<h3 id="5">5 基于图的聚类</h3>
























<h3 id="6">SOM</h3>
* 一种可用于聚类的神经网络模型.现在还没系统地学习神经网络,因此先挖个坑,日后再填.
* [Wiki对SOM的解释](https://en.wikipedia.org/wiki/Self-organizing_map)

