---
layout: post
title:  "聚类算法大盘点"
date:   2017-04-10
---
<br>最近在关注聚类分析,了解了之后才发现,原来聚类分析里已经有这么丰富的成果,因此希望对其做个较全面的总结.
<br>本文会涉及的聚类算法有:
* [1 层次(系统)聚类(Agglomerative Clustering)](#1)
<br>[1.1 凝聚层次聚类](#1.1)
<br>[1.2 分裂层次聚类](#1.2)
* [2 基于原型的聚类](#2)
<br>[2.1 K-均值(K-means)](#2.1)
<br>[2.2 二分K-均值(bisecting K-means)](#2.2)
<br>[2.3 K-中心(K-mediods)](#2.3)
<br>[2.4 模糊C均值聚类(FCM)](#2.4)
* [3 基于密度的聚类](#3)
<br>[3.1 DBSCAN](#3.1)
<br>[3.2 DENCLUE](#3.2)
<br>[3.3 OPTICS](#3.3)
* [4 基于网格的聚类](#4)
<br>[4.1 CLIQUE](#4.1)
* [5 基于图的聚类](#5)
<br>[5.1 最小生成树聚类(MST)](#5.1)
<br>[5.2 OPOSSUM](#5.2)
<br>[5.3 Chameleon](#5.3)
<br>[5.4 SNN](#5.4)
* [6 基于神经网络](#6)
<br>[6.1 SOM](#6.1)

<h3 id="1">层次聚类 Agglomerative Clustering</h3>
层次聚类也叫系统聚类,和K-means一起是最常用的聚类方式.
<br>聚类效果如下:
<br>![](http://img.blog.csdn.net/20170413164051502)
<br>它的实现方法有两种:
1. 凝聚法
<br>自下而上.从点作为个体簇开始.迭代时每一步合并两个最接近的簇,直到所有样本合并为一簇.
2. 分裂法
<br>自上而下.从包含所有样本点的某个簇开始.迭代时每一步分裂一个个体簇,直到所有簇都为个体簇.

<h4 id="1.1">凝聚法</h4>
凝聚法的关键是合并两个最接近的簇.合并的依据就是**簇间的距离**.不同的距离通常会得到不同的聚类结果.常用的有如下几种:
* 簇间最小距离
<br>![](http://img.blog.csdn.net/20170415225837726)
* 簇间最大距离
<br>![](http://img.blog.csdn.net/20170415225809085)
* 簇间平均距离
<br>![](http://img.blog.csdn.net/20170415231046834)
* 簇间质心距离
<br>![](http://img.blog.csdn.net/20170415231438555)
* Ward方法
<br>两个簇合的临近度定义为**两个簇合并时导致的平方误离差平方和的增量**.每次选取增量最小的两簇进行合并.该思想来源于方差分析:若分类正确,同类样平的离差平方和应该小,类与类之间的离差平方和应该大

我们可以看到,除Ward方法之外,簇间距离也依赖于不同簇中的点间距离.
<br>点间距离可以采用欧氏距离,马氏距离,闵氏距离等等.
<br>更多的点间距离可以看这篇文章 [传送门](https://liangyaorong.github.io/blog/2016/%E5%B8%B8%E7%94%A8%E8%B7%9D%E7%A6%BB%E6%80%BB%E7%BB%93/)

<h4 id="1.2">分裂法</h4>
分裂法与凝聚法刚好相反.每次分裂出一个个体簇.主要的实现方式是"最小生成树聚类".在这里先放着不讲.

### 层次聚类的优缺点
* 优点:
1. 聚类质量较高.
* 缺点:
1. 算法复杂度![](http://latex.codecogs.com/gif.latex?O(m^{2}logm)),空间复杂度![](http://latex.codecogs.com/gif.latex?O(m^{2})).这都相当昂贵.不利于其大规模运用.
2. 缺乏全局目标函数.

<h3 id="2">基于原型的聚类 Agglomerative Clustering</h3>
基于原型的定义是每个对象到该簇的原型的距离比到其他簇的原型的距离更近。其中,原型指样本空间中具有代表性的点.
<br>通俗地讲，就是对象离哪个簇近，这个对象就属于哪个簇。因此这种聚类的核心是如何确定簇的原型.

<h4 id="2.1">K-均值(K-means)</h4>
* 取原型为样本均值.即样本质心.K-means中的K指簇的个数.
<br>目标函数函数为:
<br>![](http://latex.codecogs.com/gif.latex?J=\sum_{i=1}^{n}\sum_{j=1}^{c} U_{i,j}d(x_{i},c_{j}))
<br>其中,
<br>![](http://latex.codecogs.com/gif.latex?U_{i,j}=
\left\{\begin{matrix}
1 & \forall k\neq j,d(x_{i},c_{j})\leqslant d(x_{i},c_{k})\\ 
0 & others
\end{matrix}\right.)
<br>可以认为,
![](http://latex.codecogs.com/gif.latex?U_{i,j})是
![](http://latex.codecogs.com/gif.latex?x_{i})所属簇的特征函数;
也可认为是样本
![](http://latex.codecogs.com/gif.latex?x_{i})隶属于簇的隶属度.隶属为1,不隶属为0.
<br>
* 算法流程如下:
<br>![](http://img.blog.csdn.net/20170417122226183)
<br>
<br>下图展示了对n个样本点进行K-Means聚类的效果，这里K取2。
<br>![](http://img.blog.csdn.net/20170417123253063)
<br>
* K-means的优缺点
<br>优点:
<br>1. 计算速度快(算法复杂度![](http://latex.codecogs.com/gif.latex?O(mk\cdot round))),原理简单.
<br>缺点:
<br>1. K难以确定.
<br>2. 受初始质心影响较大.
<br>3. 对异常值非常敏感(平均确定质心).

<h4 id="2.2">二分K-均值(bisecting K-means)</h4>
* 为克服 K-均值 算法收敛于局部最小值的问题,有人提出了另一个称为 二分K-均值 的算法.该算法首先将所有点作为一个簇,然后利用 K-means(K=2) 将该簇一分为二。之后选择其中一个簇继续进行划分,选择哪一个簇进行划分取决于对其划分是否可以最大程度降低总SSE的值。上述基于SSE的划分过程不断重复,直到得到用户指定的簇数目为止。
* 算法流程如下:
<br>![](http://img.blog.csdn.net/20170417191205595)

<h4 id="2.3">K-中心(K-mediods)</h4>
* 从K-means中我们可以看到,它对异常点非常敏感.造成这个缺点的原因在于,每轮更新质点的时候是取簇中样本的平均.
<br>要解决这个问题可以改变质点的更新方法.
<br>在 K-medoids中，我们将从当前簇中选取这样一个点作为中心点,使它到簇中其他所有点的距离之和最小。
<br>其他步骤和K-means一致.
<br>
* K-mediods的优缺点
<br>优点:
<br>1. 解决K-means对异常点敏感的问题
<br>缺点:
<br>1. 由于要对每个簇中样本点进行遍历来寻找中心点,因此计算复杂度![](http://latex.codecogs.com/gif.latex?O((mk+m)\cdot round))较K-means大.因此只适用于较小的样本.

<h4 id="2.4">模糊C均值(FCM)</h4>
上面提到的K-均值聚类,其实它有另一个名字,C-均值聚类(HCM).要讲模糊C-均值,我们先从C-均值,也就是K-均值这个角度谈起.
<br>
* HCM
<br>从上面对K-均值算法的介绍中,我们已经知道了,K-均值的目标函数是:
<br>![](http://latex.codecogs.com/gif.latex?J=\sum_{i=1}^{n}\sum_{j=1}^{c} u_{ij}d(x_{i},c_{j}))
<br>其中,
<br>![](http://latex.codecogs.com/gif.latex?u_{ij}=
\left\{\begin{matrix}
1 & \forall k\neq j,d(x_{i},c_{j})\leqslant d(x_{i},c_{k})\\ 
0 & others
\end{matrix}\right.)
<br>
<br>这也是为什么叫H(Hard)CM.要么隶属该簇,要么不隶属该簇.太硬了.
<br>引入模糊数学的观点,使得每个给定数据点用值在0，1间的隶度来确定其属于各个组的程度,便得到FCM的核心思想.
<br>
* FCM
<br>FCM的目标函数是
<br>![](http://latex.codecogs.com/gif.latex?J=\sum_{i=1}^{n}\sum_{j=1}^{c} u_{ij}^{m}d(x_{i},c_{j}))
<br>其中
<br>![](http://latex.codecogs.com/gif.latex?u_{ij})介于0,1之间;
<br>![](http://latex.codecogs.com/gif.latex?m\in [1,+\infty ))为控制算法模糊程度的参数.
<br>而且,对任意样本![](http://latex.codecogs.com/gif.latex?x_{i})满足:
<br>![](http://latex.codecogs.com/gif.latex?\sum_{j=1}^{c} u_{ij}=1)
<br>
<br>借助拉格朗日乘子,我们将限制条件整合到目标函数中得到新的目标函数
<br>
<br>![](http://latex.codecogs.com/gif.latex?J'=\sum_{i=1}^{n}\sum_{j=1}^{c} (u_{ij})^{m}d(x_{i},c_{j})+\sum_{i=1}^{n}\lambda_{i}(\sum_{j=1}^{c}u_{ij}-1))
<br>
<br>即在每一轮迭代中,要寻找![](http://latex.codecogs.com/gif.latex?u_{ij})使得目标函数最小.
<br>
<br>下面我们对其进行求解:
1. 当![](http://latex.codecogs.com/gif.latex?d_{ij})为欧氏距离,![](http://latex.codecogs.com/gif.latex?J')对中心![](http://latex.codecogs.com/gif.latex?c_{i})求偏导并令其为0,有
<br>
<br>![](http://latex.codecogs.com/gif.latex?\frac{\partial J'}{\partial c_{j}}=\sum_{i=1}^{n}(u_{ij})^{m}x_{i}-c_{j}\sum_{j=1}^{n}(u_{ij})^{m}=0 \ \ \ \ \ (1))
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow c_{j}=\frac{\sum_{i=1}^{n}(u_{ij})^{m}x_{i}}{\sum_{i=1}^{n}(u_{ij})^{m}}\ \ \ \ \ (2)) 
<br>
2. ![](http://latex.codecogs.com/gif.latex?J')对变量![](http://latex.codecogs.com/gif.latex?u_{ij})求偏导并令其为0,有:
<br>![](http://latex.codecogs.com/gif.latex?\frac{\partial J'}{\partial u_{ij}}=m(u_{ij})^{m-1}\cdot d(x_{i},c_{j})+\lambda_{j} =0 \ \ \ \ \ (3))
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow u_{ij}=\bigl(\begin{smallmatrix}
\frac{-\lambda_{j}}{m\cdot d(x_{i},c_{j})}
\end{smallmatrix}\bigr)^{\frac{1}{m-1}}\ \ \ \ \ (4))
<br>
<br>为了使公式(4)满足条件![](http://latex.codecogs.com/gif.latex?\sum_{j=1}^{c} u_{ij}=1),我们将公式(4)代入条件,得到:
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow \sum_{j=1}^{c}u_{ij}=\sum_{j=1}^{c}\bigl(\begin{smallmatrix}
\frac{(-\lambda_{j})^{\frac{1}{m-1}}}{[m\cdot d(x_{i},c_{j})]^{\frac{1}{m-1}}}
\end{smallmatrix}\bigr)=1)
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  (-\lambda_{j})^{\frac{1}{m-1}} \sum_{j=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{j}) }
\end{bmatrix}^{\frac{1}{m-1}}=1)
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  (-\lambda_{j})^{\frac{1}{m-1}}= \begin{Bmatrix}
\sum_{j=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{j}) }
\end{bmatrix}^{\frac{1}{m-1}}
\end{Bmatrix}^{-1}\ \ \ \ \ (5))
<br>
<br>看,我们得到了公式(5),即![](http://latex.codecogs.com/gif.latex?\lambda_{j})所需满足的条件.接下来将公式(5)代回到公式(4),我们得到:
<br>![](http://latex.codecogs.com/gif.latex?u_{ij}= \frac{\begin{Bmatrix}
\sum_{k=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{k}) }
\end{bmatrix}^{\frac{1}{m-1}}
\end{Bmatrix}^{-1}}{\begin{bmatrix}
m\cdot d(x_{i},c_{j})
\end{bmatrix}^{\frac{1}{m-1}}})
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\begin{bmatrix}
m\cdot d(x_{i},c_{j})
\end{bmatrix}^{\frac{1}{m-1}}
\cdot 
\begin{Bmatrix}
\sum_{k=1}^{c}\begin{bmatrix}
\frac{1}{ m\cdot d(x_{i},c_{k}) }
\end{bmatrix}^{\frac{1}{m-1}}
\end{Bmatrix}})
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\sum_{k=1}^{c}\begin{bmatrix}
\frac{m\cdot d(x_{i},c_{j})}{m\cdot d(x_{i},c_{k})}
\end{bmatrix}^{\frac{1}{m-1}}})
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\sum_{k=1}^{c}\begin{bmatrix}
\frac{d(x_{i},c_{j})}{d(x_{i},c_{k})}
\end{bmatrix}^{\frac{1}{m-1}}}\ \ \ \ \ (6))
<br>
3. 数学推导到这里就结束了,我们关键再看一下公式(2)与公式(6).这两个公式分别决定了簇中心与隶属矩阵的更新.
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow c_{j}=\frac{\sum_{i=1}^{n}(u_{ij})^{m}x_{i}}{\sum_{i=1}^{n}(u_{ij})^{m}}\ \ \ \ \ (2)) 
<br>
<br>![](http://latex.codecogs.com/gif.latex?\Rightarrow  u_{ij}=\frac{1}{\sum_{k=1}^{c}\begin{bmatrix}
\frac{d(x_{i},c_{j})}{d(x_{i},c_{k})}
\end{bmatrix}^{\frac{1}{m-1}}}\ \ \ \ \ (6))
<br>
* 模糊C均值的算法流程如下:
<br>![](http://img.blog.csdn.net/20170418202647364)
<br>
<br>当算法收敛,就可以得到聚类中心和各个样本对于各簇的隶属度值,从而完成了模糊聚类划分.如果有需要,还可以将模糊聚类结果进行去模糊化.即用一定规则把模糊聚类划分为确定性分类.

<h3 id="3">基于密度的聚类</h3>
基于密度的聚类寻找被低密度区域分离的高密度区域.
<br>对密度的不同定义衍生出不同的聚类方式
<br>
<h4 id="3.1">DBSCAN</h4>
* DBSCAN,全称为 Density-Based Spatial Clustering of Applications with Noise.从名字可以看出,它能识别噪声.当然,噪声不属于任何一类.DBSCAN用基于中心的方法定义密度.基于中心指根据中心领域内样点的数量衡量密度.
<br>下面我们先来看一下学习DBSCAN的预备知识.
* 核心点(core point): 给定邻域半径Eps(![](http://latex.codecogs.com/gif.latex?\epsilon )),样本点个数MinPts.若该点的Eps邻域内包含的样本点数>MinPts,则认为该点为核心点.
* 边界点(border point): 该点不是核心点,但其在某个核心点的Eps邻域内.
* 噪声点(noise point): 该点既非核心点,也非边界点.
* 直接密度可达:若p在q的Eps邻域内,且q为核心点,则认为,p从q出发是直接密度可达的.
* 密度可达: 若存在某一样本链![](http://latex.codecogs.com/gif.latex?p_{1},p_{2},p_{3},...,p_[n}).![](http://latex.codecogs.com/gif.latex?p_{i+1},i=1,2,...,n-1)从![](http://latex.codecogs.com/gif.latex?p_{i})出发是直接密度可达的,则认为![](http://latex.codecogs.com/gif.latex?p_{n})从![](http://latex.codecogs.com/gif.latex?p_{1})密度可达.
* 密度相连: 若p,q都是从某点O密度可达的,则认为p,q密度相连.

DBSCAN的目标是寻找最大的密度相连集合,并标记其为同一类.
<br>
<br>![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/400px-DBSCAN-Illustration.svg.png)
<br>图中A为核心点; B,C为边界点；N为噪声点;
<br>B从A出发密度可达; C从A出发密度可达；A,B,C密度相连.因此该图中{A,B,C}为一类
<br>
* DBSCAN的算法流程
![](http://img.blog.csdn.net/20170419142508199)








<h3 id="6">SOM</h3>
* 一种可用于聚类的神经网络模型.现在还没系统地学习神经网络,因此先挖个坑,日后再填.
* [Wiki对SOM的解释](https://en.wikipedia.org/wiki/Self-organizing_map)

