---
layout: post
title:  "聚类算法大盘点"
date:   2017-04-10
---
<br>最近在关注聚类分析,了解了之后才发现,原来聚类分析里已经有这么丰富的成果,因此希望对其做个较全面的总结.
<br>这篇文章会涉及非常多的聚类方法,请容许我慢慢更新.
<br>本文会涉及的聚类算法有:
* 层次(系统)聚类  Agglomerative Clustering
1. 凝聚层次聚类
2. 分裂层次聚类
* 基于原型的聚类
1. K-均值  K-means 
2. 二分K-均值  bisecting K-means 
3. K-中心  K-mediods 
4. 模糊C均值聚类  FCM 
4. SOM
* 基于密度的聚类
1. DBSCAN
2. 基于网格的聚类  CLIQUE
3. DENCLUE
* 基于图的聚类
1. 最小生成树聚类  MST
2. OPOSSUM
3. Chameleon
4. SNN

<br>
## 1  层次聚类 Agglomerative Clustering
层次聚类也叫系统聚类,和K-means一起是最常用的聚类方式.
<br>聚类效果如下:
<br>![](http://img.blog.csdn.net/20170413164051502)
<br>它的实现方法有两种:
1. 凝聚法
<br>自下而上.从点作为个体簇开始.迭代时每一步合并两个最接近的簇,直到所有样本合并为一簇.
2. 分裂法
<br>自上而下.从包含所有样本点的某个簇开始.迭代时每一步分裂一个个体簇,直到所有簇都为个体簇.

### 1.1 凝聚法
凝聚法的关键是合并两个最接近的簇.合并的依据就是**簇间的距离**.不同的距离通常会得到不同的聚类结果.常用的有如下几种:
* 簇间最小距离
<br>![](http://img.blog.csdn.net/20170415225837726)
* 簇间最大距离
<br>![](http://img.blog.csdn.net/20170415225809085)
* 簇间平均距离
<br>![](http://img.blog.csdn.net/20170415231046834)
* 簇间质心距离
<br>![](http://img.blog.csdn.net/20170415231438555)
* Ward方法
<br>两个簇合的临近度定义为**两个簇合并时导致的平方误离差平方和的增量**.每次选取增量最小的两簇进行合并.该思想来源于方差分析:若分类正确,同类样平的离差平方和应该小,类与类之间的离差平方和应该大

我们可以看到,除Ward方法之外,簇间距离也依赖于不同簇中的点间距离.
<br>点间距离可以采用欧氏距离,马氏距离,闵氏距离等等.
<br>更多的点间距离可以看这篇文章 [传送门](https://liangyaorong.github.io/blog/2016/%E5%B8%B8%E7%94%A8%E8%B7%9D%E7%A6%BB%E6%80%BB%E7%BB%93/)

### 1.2 分裂法
分裂法与凝聚法刚好相反.每次分裂出一个个体簇.主要的实现方式是"最小生成树聚类".在这里先放着不讲.

### 1.3 系统聚类的优缺点
* 优点:
1. 聚类质量较高.
* 缺点:
1. 算法复杂度![](http://latex.codecogs.com/gif.latex?O(m^{2}logm)),空间复杂度![](http://latex.codecogs.com/gif.latex?O(m^{2})).这都相当昂贵.不利于其大规模运用.
2. 缺乏全局目标函数.

## 2  基于原型的聚类
基于原型的定义是每个对象到该簇的原型的距离比到其他簇的原型的距离更近。其中,原型指样本空间中具有代表性的点.
<br>通俗地讲，就是对象离哪个簇近，这个对象就属于哪个簇。因此这种聚类的核心是如何确定簇的原型.
<br>
### 2.1 K-均值 K-means 
取原型为样本均值.即样本质心.
<br>K-means中的K指簇的个数.
<br>其算法流程如下:
<br>![](http://img.blog.csdn.net/20170417122226183)
<br>
<br>下图展示了对n个样本点进行K-Means聚类的效果，这里K取2。
<br>![](http://img.blog.csdn.net/20170417123253063)
<br>
<br>**K-means的优缺点**
<br>
* 优点:
1. 计算速度快(算法复杂度![](http://latex.codecogs.com/gif.latex?O(mk*round))),原理简单.
* 缺点:
1. K难以确定.
2. 受初始质心影响较大.
3. 对异常值非常敏感(平均确定质心).

### 2.2 二分K-均值 bisecting K-means 
为克服 K-均值 算法收敛于局部最小值的问题,有人提出了另一个称为 二分K-均值 的算法.该算法首先将所有点作为一个簇,然后利用 K-means(K=2) 将该簇一分为二。之后选择其中一个簇继续进行划分,选择哪一个簇进行划分取决于对其划分是否可以最大程度降低总SSE的值。上述基于SSE的划分过程不断重复,直到得到用户指定的簇数目为止。
<br>
<br>流程图如下:
<br>![](http://img.blog.csdn.net/20170417191205595)
<br>
### 2.3 K-中心 K-mediods 
从K-means中我们可以看到,它对异常点非常敏感.造成这个缺点的原因在于,每轮更新质点的时候是取簇中样本的平均.
<br>要解决这个问题可以改变质点的更新方法.
<br>在 K-medoids中，我们将从当前簇中选取这样一个点作为中心点,使它到簇中其他所有点的距离之和最小。
<br>其他步骤和K-means一致.
<br>
<br>**K-mediods的优缺点**
* 优点:
1. 解决K-means对异常点敏感的问题
* 缺点:
1. 由于要对每个簇中样本点进行遍历来寻找中心点,因此计算复杂度![](http://latex.codecogs.com/gif.latex?O((mk+m)*round))较K-means大.因此只适用于较小的样本.
2. K难以确定.
3. 受初始质心影响较大.

### 2.4 模糊C均值聚类 FCM










