---
layout: post
title:  "GBDT的具体实现"
date:   2017-03-28
---
<br>上一篇文章中我们已经大概了解了Gradient Boosting的来源和主要数学思想。在这篇文章里，我们将以sklearn中的Gradient Boosting为基础 [源码在这](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/ensemble/gradient_boosting.py#L1635)，了解GBDT的实现过程.希望大家能在看这篇文章的过程中有所收获.
<br>这里面会有大量的代码，请耐住性子,我们一起把它啃下来.
<br>
## GBDT
### 什么是GBDT
<br>
<br>GBDT全称是Gradient Boosting Decision Trees，顾名思义，就是在梯度提升框架下(上一篇文章有详细解说[传送门](https://liangyaorong.github.io/blog/2017/Boosting/))，用回归树作为基本分类器的算法(分类树相加会有问题，如男+女=?).因此可以想到，参数调优时有两方面：
<br>1.梯度提升框架方面：
* “损失函数 loss”
* “步长 learning_rate”
* “迭代次数 n_estimators”
* “样本权重”

<br>2.决策树方面：
* “最大深度max_depth”
* “特征分割标准criterion(默认为friedman_mse)”
* “最小样本分割次数min_samples_split ”
* “叶子中最小样本数min_samples_leaf ”
* “特征选取数max_features”
* “最大叶子数max_leaf_nodes ”
* “重采样比例subsample”...

<br>大家可以参考sklearn的用户手册[Guide](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)
<br>
<br>为什么要用决策树作为基本分类器？
<br>决策树优点在于可理解性强，可以快速生成我们能理解的规则，而且计算量相对而言不大(每棵树的复杂度为O(mnlog(Depth))).
<br>但决策树有一个很大的缺点:高方差和不稳定.由于决策数是运用启发式生成的，初始分割点的一点改变就会导致最后结果完全不同.这使得决策树对特征的要求非常高.但实际中特征噪声是很多的.
<br>因此它是典型的弱分类器.
<br>
### GBDT的中的数学
<br>
<br>当我们的基本分类器是一个包含J个节点的回归树时，回归树模型可以表示为
<br>![](http://latex.codecogs.com/gif.latex?h(x;\{b_j, R_j\}_1^J) = \sum_{b=j}^Jb_jI(x\in R_j) \qquad)
<br>其中Rj为不相交的区域，它们的集合覆盖了预测值空间，bj是叶子节点的值.
<br>因此，在回归树为基模型下，算法最终的结果为：
<br>![](http://latex.codecogs.com/gif.latex?$$F_m(x)=F_{m-1}(x) + \rho_m \sum_{j=1}^J b_{jm}I(x \in R_{jm})\qquad)
<br>通俗的理解就是这样：
<br>![](http://img.blog.csdn.net/20170329191224077)
<br>说白了就是每次的预测相加.不难理解吧
<br>
## 实现代码
<br>了解了最基本的构造，我们接着看一下GBDT的代码实现
<br>
### 片段一（回归树叶子节点估计，以取平均为例）
```python
class MeanEstimator(BaseEstimator):
    """An estimator predicting the mean of the training targets."""
    def fit(self, X, y, sample_weight=None):
        if sample_weight is None:
            self.mean = np.mean(y)
        else:
            self.mean = np.average(y, weights=sample_weight)

    def predict(self, X):
        check_is_fitted(self, 'mean')

        y = np.empty((X.shape[0], 1), dtype=np.float64)
        y.fill(self.mean)
	return y
```
<br>若无样本权重，则简单求和;若有样本权重，则加权求和
<br>除平均之外，叶子节点取值方式还有：
* 分位数
* 对数优势比(log Odd Ratio)
* 先验概率
* 零填充

### 片段二(损失函数,以平方损失为例)
```python
class LeastSquaresError(RegressionLossFunction):
    def init_estimator(self):
        return MeanEstimator()

    def __call__(self, y, pred, sample_weight=None):
        if sample_weight is None:
            return np.mean((y - pred.ravel()) ** 2.0)
        else:
            return (1.0 / sample_weight.sum() *
                    np.sum(sample_weight * ((y - pred.ravel()) ** 2.0)))

    def negative_gradient(self, y, pred, **kargs):
        return y - pred.ravel()

    def update_terminal_regions(self, tree, X, y, residual, y_pred,
                                sample_weight, sample_mask,
                                learning_rate=1.0, k=0):
        """Least squares does not need to update terminal regions.
        But it has to update the predictions.
        """
        # update predictions
        y_pred[:, k] += learning_rate * tree.predict(X).ravel()

    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight):
	pass
```
<br>我们可以看到损失函数类有三个方法：
* 求损失
* 求负梯度
* 将预测集在该轮训练得到的结果添加到最终预测中

<br>除平方损失之外，
<br>对于Regressor还有
* 绝对损失
* Huber损失
* 分位数损失

<br>对于classifier有
* 二项偏差(Binomial Deviance)
* 多项偏差(Multinomial Deviance)
* 指数损失(Exponential Loss)

### 片段三(GBDT的训练 _fit_stages)
```python
class BaseGradientBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):
    """Abstract base class for Gradient Boosting. """

    @abstractmethod
    def __init__(...):
        ...
        ...

    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,
                    begin_at_stage=0, monitor=None, X_idx_sorted=None):
        """Iteratively fits the stages.

        For each stage it computes the progress (OOB, train score)
        and delegates to ``_fit_stage``.
        Returns the number of stages fit; might differ from ``n_estimators``
        due to early stopping.
        """
1->   n_samples = X.shape[0]
        do_oob = self.subsample < 1.0
        sample_mask = np.ones((n_samples,), dtype=np.bool)
        n_inbag = max(1, int(self.subsample * n_samples))
        loss_ = self.loss_

        # Set min_weight_leaf from min_weight_fraction_leaf
        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
            min_weight_leaf = (self.min_weight_fraction_leaf *
                               np.sum(sample_weight))
        else:
            min_weight_leaf = 0.
        if self.verbose:
            verbose_reporter = VerboseReporter(self.verbose)
            verbose_reporter.init(self, begin_at_stage)

        X_csc = csc_matrix(X) if issparse(X) else None
        X_csr = csr_matrix(X) if issparse(X) else None

        # perform boosting iterations
        i = begin_at_stage
2->   for i in range(begin_at_stage, self.n_estimators):

            # subsampling
            if do_oob:
3->           sample_mask = _random_sample_mask(n_samples, n_inbag,random_state)

                # OOB score before adding this stage
4->           old_oob_score = loss_(y[~sample_mask],
                                      y_pred[~sample_mask],
                                      sample_weight[~sample_mask])

            # fit next stage of trees
5->       y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,
                                     sample_mask, random_state, X_idx_sorted,
                                     X_csc, X_csr)

            # track deviance (= loss)
6->       if do_oob:
                self.train_score_[i] = loss_(y[sample_mask],  #更新train_score_
                                             y_pred[sample_mask],
                                             sample_weight[sample_mask])
                self.oob_improvement_[i] = (
                    old_oob_score - loss_(y[~sample_mask],
                                          y_pred[~sample_mask],
                                          sample_weight[~sample_mask]))
7->       else:
                # no need to fancy index w/ no subsampling
                self.train_score_[i] = loss_(y, y_pred, sample_weight)

             #update verbose_reporter
8->        if self.verbose > 0:
                verbose_reporter.update(i, self)

            #update monitor
9->       if monitor is not None:
                early_stopping = monitor(i, self, locals())
                if early_stopping:
                    break
        return i + 1
```
<br>根据代码中的标号，我们一步一步来看
* 1.训练前的初始化，以及做好输入检测
* 2.进入T轮迭代，T=n_estimators. 注意，若采用重采样进行训练，若OOB中出现过拟合，会提前停止迭代(由monitor决定)
 - 3.确定训练样本
 - 4.计算原始oob error
 - 5.调用_fit_stage进行当前轮训练并返回最新y_pred(！重点！)
 - 6.计算oob_improvement_
 - 7.更新train_score_
 - 8.更新verbose_reporter
 - 9.更新monitor，判断是否要提前停止迭代

<br>是否很好奇，流程5是怎么实现的？我们接着看
<br>
### 片段四(每一轮的训练 _fit_stage)
```python
def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
               random_state, X_idx_sorted, X_csc=None, X_csr=None):
    """Fit another stage of ``n_classes_`` trees to the boosting model. """

    assert sample_mask.dtype == np.bool  # if flase, raise error
    loss = self.loss_
    original_y = y

    for k in range(loss.K):
        if loss.is_multi_class:
            y = np.array(original_y == k, dtype=np.float64)

1->     residual = loss.negative_gradient(y, y_pred, k=k,
                                          sample_weight=sample_weight)

        # induce regression tree on residuals
2->     tree = DecisionTreeRegressor(
            criterion=self.criterion,
            splitter='best',
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            min_samples_leaf=self.min_samples_leaf,
            min_weight_fraction_leaf=self.min_weight_fraction_leaf,
            min_impurity_split=self.min_impurity_split,
            max_features=self.max_features,
            max_leaf_nodes=self.max_leaf_nodes,
            random_state=random_state,
            presort=self.presort)

3->     if self.subsample < 1.0:
            # no inplace multiplication!
            sample_weight = sample_weight * sample_mask.astype(np.float64)

4->     if X_csc is not None:
            tree.fit(X_csc, residual, sample_weight=sample_weight,
                     check_input=False, X_idx_sorted=X_idx_sorted)
        else:
            tree.fit(X, residual, sample_weight=sample_weight,
                     check_input=False, X_idx_sorted=X_idx_sorted)

        # update tree leaves
5->     if X_csr is not None:
            loss.update_terminal_regions(tree.tree_, X_csr, y, residual, y_pred,
                                         sample_weight, sample_mask,
                                         self.learning_rate, k=k)
        else:
            loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
                                         sample_weight, sample_mask,
                                         self.learning_rate, k=k)

6->     # add tree to ensemble
        self.estimators_[i, k] = tree

7-> return y_pred

```
<br>假设大家已经对决策树的生成与预测有所了解，这篇文章里就不展开讲了.想看决策树基础代码的同学可以看一下这个[传送门](https://github.com/liangyaorong/Basic_Algorithm/blob/master/ML/Regression_Tree.py)（代码来自《机器学习实战》，我加了一些注释）
<br>
<br>函数_fit_stage展示的是GBDT每一轮里的训练过程.主要分为7部分(标号).
* 1.计算负梯度
* 2.初始化基模型(GBDT的基模型就是决策树)
* 3.若用子样本训练，则获取样本权重对应子集
* 4.用基模型进行该轮训练
* 5.更新预测空间的划分及其取值
* 6.讲该轮模型储藏起来，用于以后的泛化
* 7.返回最新的预测值

<br>注意，这里面有一个for循环，其实是针对分类的(二项分类与多项分类).
<br>若是分类，模型会根据标签(label)类别对y进行重构，相同的一label放在一列.在训练中对每一列分别进行训练.
<br>若是连续型的预测，很显然n_classes_ == 1,即进入一次循环就好了
<br>
<br>其实也不是很复杂对不对.整个过程要注意的细节就是子样本的采样和分类器的训练
<br>当然还有一些细节如：
* verbose_reporter的实现
* monitor的实现

这些就留给你们自己去研究了.看懂了收益也会很大吧
### 片段五(GBDT的预测 _fit_stages)
<br>待续...
