---
layout: post
title:  "GBDT的具体实现"
date:   2017-03-28
---
<br>上一篇文章中我们已经大概了解了Gradient Boosting的来源和主要数学思想。在这篇文章里，我们将以sklearn中的Gradient Boosting为基础 [源码在这](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/ensemble/gradient_boosting.py#L1635)，了解GBDT的实现过程.希望大家能在看这篇文章的过程中有所收获.
<br>这里面会有大量的代码与公式，请耐住性子,我们一起把它啃下来.
<br>
### GBDT
<br>**什么是GBDT**
<br>GBDT全称是Gradient Boosting Decision Trees，顾名思义，就是在梯度提升框架下(上一篇文章有详细解说[传送门](https://liangyaorong.github.io/blog/2017/Boosting/))，用回归树作为基本分类器的算法(分类树相加会有问题，如男加女...).因此可以想到，参数调优时有两方面：
<br>1.梯度提升框架方面：“损失函数 loss”，“步长 learning_rate”，“迭代次数 n_estimators”，“样本权重(sklearn中不允许样本权重调整，即等权重)”
<br>2.决策树方面：“最大深度max_depth”，“特征分割标准criterion(默认为friedman_mse)”,“最小样本分割次数min_samples_split ”，“叶子中最小样本数min_samples_leaf ”，“特征选取数max_features”,“最大叶子数max_leaf_nodes ”，“重采样比例subsample”...
<br>大家可以参考sklearn的用户手册[Guide](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)
<br>
<br>那么为什么要用决策树作为基本分类器呢？
<br>决策树优点在于可理解性强，可以快速生成我们能理解的规则，而且计算量相对而言不大(每棵树的复杂度为O(mnlog(Depth))).但决策树有一个很大的缺点:高方差和不稳定.由于决策数是运用启发式生成的，初始分割点的一点改变就会导致最后结果完全不同.这使得决策树对特征的要求非常高.但实际中特征噪声是很多的.因此它是典型的弱分类器.
<br>
<br>**GBDT的中的数学**
<br>当我们的基本分类器是一个包含J个节点的回归树时，回归树模型可以表示为
<br>![](http://latex.codecogs.com/gif.latex?h(x;\{b_j, R_j\}_1^J) = \sum_{b=j}^Jb_jI(x\in R_j) \qquad）
<br>其中Rj为不相交的区域，它们的集合覆盖了预测值空间，bj是叶子节点的值.
<br>因此，在回归树为基模型下，算法最终的结果为：
<br>![](http://latex.codecogs.com/gif.latex?$$F_m(x)=F_{m-1}(x) + \rho_m \sum_{j=1}^J b_{jm}I(x \in R_{jm})\qquad)
<br>通俗的理解就是这样：
<br>![](http://img.blog.csdn.net/20170329191224077)
<br>说白了就是每次的预测相加
### 实现代码
<br>片段一(损失函数,以平方损失为例)：
```python
class LeastSquaresError(RegressionLossFunction):
    def init_estimator(self):
        return MeanEstimator()

    def __call__(self, y, pred, sample_weight=None):
        if sample_weight is None:
            return np.mean((y - pred.ravel()) ** 2.0)
        else:
            return (1.0 / sample_weight.sum() *
                    np.sum(sample_weight * ((y - pred.ravel()) ** 2.0)))

    def negative_gradient(self, y, pred, **kargs):
        return y - pred.ravel()

    def update_terminal_regions(self, tree, X, y, residual, y_pred,
                                sample_weight, sample_mask,
                                learning_rate=1.0, k=0):
        """Least squares does not need to update terminal regions.
        But it has to update the predictions.
        """
        # update predictions
        y_pred[:, k] += learning_rate * tree.predict(X).ravel()

    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight):
	pass
```

<br>片段二(GBDT的训练)：
```python
def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc=None, X_csr=None):
    assert sample_mask.dtype == np.bool
    loss = self.loss_
    original_y = y
    
    for k in range(loss.K):
        if loss.is_multi_class:
            y = np.array(original_y == k, dtype=np.float64)
        residual = loss.negative_gradient(y, y_pred, k=k, sample_weight=sample_weight)
        # induce regression tree on residuals
        
        tree = DecisionTreeRegressor(
            criterion=self.criterion,
            splitter='best',
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            min_samples_leaf=self.min_samples_leaf,
            min_weight_fraction_leaf=self.min_weight_fraction_leaf,
            max_features=self.max_features,
            max_leaf_nodes=self.max_leaf_nodes,
            random_state=random_state,
            presort=self.presort)
        
        if self.subsample < 1.0:
            # no inplace multiplication!
            sample_weight = sample_weight * sample_mask.astype(np.float64)
            
        if X_csc is not None:
            tree.fit(X_csc, residual, sample_weight=sample_weight,
                     check_input=False, X_idx_sorted=X_idx_sorted)
        else:
            tree.fit(X, residual, sample_weight=sample_weight,
                     check_input=False, X_idx_sorted=X_idx_sorted)
            
        # update tree leaves
        if X_csr is not None:
            loss.update_terminal_regions(tree.tree_, X_csr, y, residual, y_pred,
                                         sample_weight, sample_mask,
                                         self.learning_rate, k=k)
        else:
            loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
                                         sample_weight, sample_mask,
                                         self.learning_rate, k=k)
        # add tree to ensemble
        self.estimators_[i, k] = tree
return y_pred
```






