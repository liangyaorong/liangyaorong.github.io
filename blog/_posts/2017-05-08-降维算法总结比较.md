---
layout: post
title:  "特征降维算法总结比较"
date:   2017-05-08
---

特征降维有利于减少描述
分为特征提取与特征的选择.这两者是有区别的
<br>特征提取指对特征进行某种变换,得到新特征; 特征选择指通过某些方法获取特征的子集.
这篇文章是对以上两者的总结.

* [1 特征提取 (Feature extration)](#1)
  * [1.1 主成分分析 (Principal Component Analysis, PCA)](#1.1)
  * [1.2 核主成分分析 (Kernel Principal Component Analysis, KPCA)](#1.2)
  * [1.3 线性判别分析 (Linear Discriminant Analysis, LDA)](#1.3)
  * [1.4 局部线性嵌入 (Locally Linear Embedding, LLE)](#1.4)
	
* [2 特征选择 (Feature selection)](#2)
  * [2.1 剔除低方差特征 (Removing features with low variance)](#2.1)
  * [2.2 单变量特征选择 (Univariate feature selection)](#2.2)
<br>[2.2.1 卡方检验 (chi_square)](#2.2.1)
<br>[2.2.2 互信息 (mutual_info)](#2.2.2)
<br>[2.2.3 皮尔逊相关系数 (Pearson correlation coefficient)](#2.2.3)
  * [2.3 从模型中进行特征选择 (select from model)](#2.3)
<br>[2.3.1 L1正则化 (L1-based feature selection)](#2.3.1)
<br>[2.3.2 随机森林 (Tree-based feature selection)](#2.3.2)





<h3 id="1">1 特征提取(Feature extration)</h3>
特征提取指通过特征组合的方式生成新特征的过程.这个组合可以是线性的(如,PCA),也可以是非线性的(如,PCA的非线性推广:KPCA)
<h4 id="1.1">1.1 主成分分析 (Principal Component Analysis, PCA)</h4>

<h4 id="1.2">1.2 核主成分分析 (Kernel Principal Component Analysis, KPCA)</h4>

<h4 id="1.3">1.3 线性判别分析 (LDA)</h4>
LDA与PCA最大的差别在于:PCA是无监督的,而LDA是有监督的; PCA着重描述特征,而LDA着重抓住其判别特征; 因此PCA变换矩阵是正交的,而LDA一般不是正交的(并不关注).

* 线性判别分析的核心思想
<br>通过线性变换,使得变换后的矩阵对应的,不同类别的样本尽可能地分开.
<br>什么叫尽可能分开呢?可以从两个方面来衡量.
<br>1. 各类样本的类间方差尽可能大.
<br>![](http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104212324555025.jpg)
<br>右方两类的样本中心较左方相隔更远,因此分类效果更好.
<br>
<br>2. 各类样本的类内方差尽可能小.
<br>![](http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104212324566496.png)
<br>光考虑类间中心距离还是不够的.从上图可以看到,若投影到x1上,虽然两类中心相比投影到x2上,相隔较远.但由于两类在x1上的方差都很大,因此从分类角度看,效果比不上投影到x2上.
<br>
<br>因此,必须同时考虑以上两方面才能达到我们想要的效果.
<br>看,这其实就是方差分析的思想. 因此目标函数可以取统计意义上的F值.F值越大越好.

* 目标优化函数
<br>接下来我们要想,怎么量化这两个标准.
<br>原始特征矩阵![](http://latex.codecogs.com/gif.latex?X_{p \times n});
<br>转换矩阵![](http://latex.codecogs.com/gif.latex?W_{p \times d});
<br>线性变换后的新矩阵![](http://latex.codecogs.com/gif.latex?Y_{d \times n} = {W}'_{p \times d}\cdot X_{p \times n}).
<br>其中,
<br>n为样本数, p为特征数, d为变换后的特征数.

  * 使变换后各类样本的类间方差尽可能大.
<br>变换后的各类样本中心:
<br>![](http://latex.codecogs.com/gif.latex?\widetilde{\mu_{j}} = \frac{1}{n_{j}}\sum_{y \in \omega _{j}} y)
<br>其中,
<br>![](http://latex.codecogs.com/gif.latex?\omega _{j})指第j类；
![](http://latex.codecogs.com/gif.latex?n_{j})指第j类的样本数.
<br>
<br>变换后的总体样本中心:
<br>![](http://latex.codecogs.com/gif.latex?\widetilde{\mu}=\frac{1}{n}\sum_{i=1}^{n}y_{i}=\frac{1}{n}\sum_{i=1}^{n}w_{i}\cdot X)
<br>
<br>因此,变换后样本类间协方差矩阵 (Between-Class Scatter Matrix)可表示为:
<br>![](http://latex.codecogs.com/gif.latex?\widetilde{S_{b}}=\sum _{j=1}^{c}\frac{n_{j}}{n}(\widetilde{\mu_{j}}-\widetilde{\mu})(\widetilde{\mu_{j}}-\widetilde{\mu})')
<br>显然,这是一个![](http://latex.codecogs.com/gif.latex?d \times d)维的矩阵

  * 使变换各类样本的类内方差尽可能小.
<br>样本类内协方差矩阵 (Within-Class Scatter Matrix)可表示为:
<br>![](http://latex.codecogs.com/gif.latex?\widetilde{S_{w}}=\sum _{j=1}^{c}\sum_{y \in \omega _{j}} (y-\widetilde{\mu_{j}})(y-\widetilde{\mu_{j}})')


  * 目标函数
<br>![](http://latex.codecogs.com/gif.latex?J(W) = \frac{|\widetilde{S_{b}}|}{|\widetilde{S_{w}}|})
<br>这里要注意到,我们对![](http://latex.codecogs.com/gif.latex?\widetilde{S_{b}}),![](http://latex.codecogs.com/gif.latex?\widetilde{S_{w}})取了行列式.因为我们希望用一个实数代替协方差矩阵.
<br>这个问题在统计学里也是有遇到的:把F分布推广到多元的Wilks分布.主要方法有:取行列式; 取迹; 取二范数等.最常用的是取行列式,由T.W.Anderson于1958年提出.尴尬的是, LDA由Fisher在1936年提出:)

  * 目标函数的展开
<br>可以看到,我们现在的目标函数是由线行变换后的Y出发推出来的.我们更希望计算时能从X出发,因此,我们把![](http://latex.codecogs.com/gif.latex?Y=W'X)代入目标函数.
<br>可得:
<br>1. ![](http://latex.codecogs.com/gif.latex?\widetilde{\mu} = W'\mu)
<br>![](http://latex.codecogs.com/gif.latex?\begin{align*}
proof: \\
\widetilde{\mu} &= \frac{1}{n}\sum_{i=1}^{n} y_{i}
&=\frac{1}{n}\sum_{i=1}^{n} W'x_{i}
&=W'\cdot \frac{1}{n}\sum_{i=1}^{n} W'x_{i}
&=W'\mu
\end{align*})
<br>
<br>2. ![](http://latex.codecogs.com/gif.latex?\widetilde{\mu_{j}} = W'\mu_{j})
<br>![](http://latex.codecogs.com/gif.latex?\begin{align*}
proof: \\
\widetilde{\mu_{j}} &= \frac{1}{n_{j}}\sum_{y \in \omega _{j}} y
&=\frac{1}{n_{j}}\sum_{x \in \omega _{j}}W'x
&=W'\cdot \frac{1}{n_{j}}\sum_{x \in \omega _{j}}x
&=W'\mu_{j}
\end{align*})
<br>
<br>3. ![](http://latex.codecogs.com/gif.latex?\widetilde{S_{b}} = W'S_{b}W'), 
其中,![](http://latex.codecogs.com/gif.latex?S_{b})为原样本类间协方差矩阵
<br>![](http://latex.codecogs.com/gif.latex?\begin{align*}
proof: \\
\widetilde{S_{b}}&=\sum _{j=1}^{c}\frac{n_{j}}{n}(\widetilde{\mu_{j}}-\widetilde{\mu})(\widetilde{\mu_{j}}-\widetilde{\mu})'\\
&=\sum _{j=1}^{c}\frac{n_{j}}{n}(W'\mu_{j}-W'\mu)(W'\mu_{j}-W'\mu)'\\
&=W'[\sum _{j=1}^{c}\frac{n_{j}}{n}(\mu_{j}-\mu)(\mu_{j}-\mu)']W\\
&=W'S_{b}W
\end{align*})
<br>
<br>4. ![](http://latex.codecogs.com/gif.latex?\widetilde{S_{w}} = W'S_{w}W), 
其中,![](http://latex.codecogs.com/gif.latex?S_{w})为原样本类内协方差矩阵
<br>![](http://latex.codecogs.com/gif.latex?\begin{align*}
proof: \\
\widetilde{S_{w}}&=\sum _{j=1}^{c}\sum_{y \in \omega _{j}} (y-\widetilde{\mu_{j}})(y-\widetilde{\mu_{j}})'\\
&=\sum _{j=1}^{c}\sum_{x \in \omega _{j}} (W'x-W'\mu_{j})(W'x-W'\mu_{j})'\\
&=W'[\sum _{j=1}^{c}\sum_{x \in \omega _{j}} (x-\mu_{j})(x-\mu_{j})']W\\
&=W'S_{w}W
\end{align*})
<br>
<br>5. 易得最终的目标优化函数:
<br>![](http://latex.codecogs.com/gif.latex?J(W) = \frac{|W'S_{b}W|}{|W'S_{w}W|}).该值越大越好.

* 目标优化函数求解
<br>我们有了目标函数![](http://latex.codecogs.com/gif.latex?J(W)),接下来我们需要对其进行求解.
<br>注意到,对变换矩阵W扩大C倍(C>0),方向没有改变,但![](http://latex.codecogs.com/gif.latex?\widetilde{S_{w}}), ![](http://latex.codecogs.com/gif.latex?\widetilde{S_{b}})却可以因此变得任意大.
<br>因此我们可以只考虑![](http://latex.codecogs.com/gif.latex?|W'S_{w}W|=1})时的情况,即对目标函数添加了一个条件.
<br>
<br>现在我们的目标函数变成了:
<br>![](http://latex.codecogs.com/gif.latex?W = argmax\ |W'S_{b}W|)
<br>![](http://latex.codecogs.com/gif.latex?s.t.\ |W'S_{w}W|=1)
<br>
<br>














<h4 id="1.4">1.4 局部线性嵌入 (LLE)</h4>

