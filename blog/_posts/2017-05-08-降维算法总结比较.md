---
layout: post
title:  "特征降维算法总结比较"
date:   2017-05-08
---

特征降维有利于减少描述
分为特征提取与特征的选择.这两者是有区别的
<br>特征提取指对特征进行某种变换,得到新特征; 特征选择指通过某些方法获取特征的子集.
这篇文章是对以上两者的总结.

* [1 特征提取 (Feature extration)](#1)
  * [1.1 主成分分析 (Principal Component Analysis, PCA)](#1.1)
  * [1.2 核主成分分析 (Kernel Principal Component Analysis, KPCA)](#1.2)
  * [1.3 线性判别分析 (Linear Discriminant Analysis, LDA)](#1.3)
  * [1.4 局部线性嵌入 (Locally Linear Embedding, LLE)](#1.4)
	
* [2 特征选择 (Feature selection)](#2)
  * [2.1 剔除低方差特征 (Removing features with low variance)](#2.1)
  * [2.2 单变量特征选择 (Univariate feature selection)](#2.2)
<br>[2.2.1 卡方检验 (chi_square)](#2.2.1)
<br>[2.2.2 互信息 (mutual_info)](#2.2.2)
<br>[2.2.3 皮尔逊相关系数 (Pearson correlation coefficient)](#2.2.3)
  * [2.3 从模型中进行特征选择 (select from model)](#2.3)
<br>[2.3.1 L1正则化 (L1-based feature selection)](#2.3.1)
<br>[2.3.2 随机森林 (Tree-based feature selection)](#2.3.2)





<h3 id="1">1 特征提取(Feature extration)</h3>
特征提取指通过特征组合的方式生成新特征的过程.这个组合可以是线性的(如,PCA),也可以是非线性的(如,PCA的非线性推广:KPCA)
<h4 id="1.1">1.1 主成分分析 (Principal Component Analysis, PCA)</h4>

<h4 id="1.2">1.2 核主成分分析 (Kernel Principal Component Analysis, KPCA)</h4>

<h4 id="1.3">1.3 线性判别分析 (LDA)</h4>
LDA与PCA最大的差别在于:PCA是无监督的,而LDA是有监督的; PCA着重描述特征,而LDA着重抓住其判别特征.

* 线性判别分析的核心思想
<br>通过线性变换,使得变换后的矩阵对应的,不同类别的样本尽可能地分开.
<br>什么叫尽可能分开呢?可以从两个方面来衡量.
<br>1. 各类样本的类间方差尽可能大.
<br>![](http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104212324555025.jpg)
<br>右方两类的样本中心较左方相隔更远,因此分类效果更好.
<br>
<br>2. 各类样本的类内方差尽可能小.
<br>![](http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104212324566496.png)
<br>光考虑类间中心距离还是不够的.从上图可以看到,若投影到x1上,虽然两类中心相比投影到x2上,相隔较远.但由于两类在x1上的方差都很大,因此从分类角度看,效果比不上投影到x2上.
<br>
<br>因此,必须同时考虑以上两方面才能达到我们想要的效果.
<br>看,这其实就是方差分析的思想. 因此目标函数可以取统计意义上的F值.F值越大越好.

* 目标优化函数
<br>接下来我们要想,怎么量化这两个标准.
<br>原始特征矩阵![](http://latex.codecogs.com/gif.latex?X_{p \times n});
<br>转换矩阵![](http://latex.codecogs.com/gif.latex?W_{p \times d});
<br>线性变换后的新矩阵![](http://latex.codecogs.com/gif.latex?Y_{d \times n} = {W}'_{p \times d}\cdot X_{p \times n}).
<br>其中,
<br>n为样本数, p为特征数, d为变换后的特征数.

  * 使各类样本的类间方差尽可能大.
<br>变换后的各类样本中心:
<br>![](http://latex.codecogs.com/gif.latex?\widetilde{\mu_{j}} = \frac{1}{n_{j}}\sum_{y \in \omega _{j}} y)
<br>其中,
<br>![](http://latex.codecogs.com/gif.latex?\omega _{j})指第j类；
![](http://latex.codecogs.com/gif.latex?n_{j})指第j类的样本数.
<br>
<br>变换后的总体样本中心:
<br>![](http://latex.codecogs.com/gif.latex?\widetilde{\mu}=\frac{1}{n}\sum_{i=1}^{n}y_{i}=\frac{1}{N}\sum_{i=1}^{n}w_{i}\cdot X)
<br>
<br>因此,样本类间方差可表示为:
<br>![](http://latex.codecogs.com/gif.latex?\widetilde{S_{B}}=\sum _{j=1}^{c}\frac{n_{j}}{n}(\widetilde{\mu_{j}}-\widetilde{\mu})(\widetilde{\mu_{j}}-\widetilde{\mu})')
<br>显然,这是一个![](http://latex.codecogs.com/gif.latex?d \times d)维的矩阵

  * 使各类样本的类内方差尽可能小.
<br>样本类内方差可表示为:
<br>![](http://latex.codecogs.com/gif.latex?\widetilde{S_{W}}=\sum _{j=1}^{c}\sum_{y \in \omega _{j}} (y-\widetilde{\mu_{j}})(y-\widetilde{\mu_{j}})')


  * 目标函数
<br>![](http://latex.codecogs.com/gif.latex?J_{W} = \frac{|\widetilde{S_{B}}|}{|\widetilde{S_{W}}|})
<br>这里要注意到,我们对![](http://latex.codecogs.com/gif.latex?\widetilde{S_{B}}),![](http://latex.codecogs.com/gif.latex?\widetilde{S_{W}})取了行列式.因为我们希望用一个实数代替矩阵.
<br>这个问题在统计学里也是有遇到的:把F分布推广到多元的Wilks分布.主要方法有:取行列式; 取迹; 取二范数等.最常用的是取行列式,是由T.W.Anderson于1958年提出.

  * 目标函数的展开
<br>我们可以看到






<h4 id="1.4">1.4 局部线性嵌入 (LLE)</h4>

