---
layout: post
title:  "特征降维算法总结比较"
date:   2017-05-08
---

特征降维有利于减少描述
分为特征提取与特征的选择.这两者是有区别的
<br>特征提取指对特征进行某种变换,得到新特征; 特征选择指通过某些方法获取特征的子集.
这篇文章是对以上两者的总结.

* [1 特征提取 (Feature extration)](#1)
  * [1.1 主成分分析 (Principal Component Analysis, PCA)](#1.1)
  * [1.2 核主成分分析 (Kernel Principal Component Analysis, KPCA)](#1.2)
  * [1.3 线性判别分析 (Linear Discriminant Analysis, LDA)](#1.3)
  * [1.4 局部线性嵌入 (Locally Linear Embedding, LLE)](#1.4)
	
* [2 特征选择 (Feature selection)](#2)
  * [2.1 剔除低方差特征 (Removing features with low variance)](#2.1)
  * [2.2 单变量特征选择 (Univariate feature selection)](#2.2)
<br>[2.2.1 卡方检验 (chi_square)](#2.2.1)
<br>[2.2.2 互信息 (mutual_info)](#2.2.2)
<br>[2.2.3 皮尔逊相关系数 (Pearson correlation coefficient)](#2.2.3)
  * [2.3 从模型中进行特征选择 (select from model)](#2.3)
<br>[2.3.1 L1正则化 (L1-based feature selection)](#2.3.1)
<br>[2.3.2 随机森林 (Tree-based feature selection)](#2.3.2)





<h3 id="1">1 特征提取(Feature extration)</h3>
特征提取指通过特征组合的方式生成新特征的过程.这个组合可以是线性的(如,PCA),也可以是非线性的(如,PCA的非线性推广:KPCA)
<h4 id="1.1">1.1 主成分分析 (Principal Component Analysis, PCA)</h4>

<h4 id="1.2">1.2 核主成分分析 (Kernel Principal Component Analysis, KPCA)</h4>

<h4 id="1.3">1.3 线性判别分析 (LDA)</h4>
LDA与PCA最大的差别在于:PCA是无监督的,而LDA是有监督的.

* 线性判别分析的核心思想
<br>通过线性变换,使得变换后的矩阵对应的,不同类别的样本尽可能地分开.
<br>什么叫尽可能分开呢?可以从两个方面来衡量.
<br>1. 各类样本中心相隔距离尽可能远.
<br>![](http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104212324555025.jpg)
<br>右方两类的样本中心较左方相隔更远,因此分类效果更好.
<br>
<br>2. 各类样本的类内方差尽可能小.
<br>![](http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104212324566496.png)
<br>光考虑类间中心距离还是不够的.从上图可以看到,若投影到x1上,虽然两类中心相比投影到x2上,相隔较远.但由于两类在x1上的方差都很大,因此从分类角度看,效果比不上投影到x2上.
<br>
<br>因此,必须同时考虑以上两方面才能达到我们想要的效果.

* 目标优化函数
接下来我们要想,怎么量化这两个标准.
<br>先以二分类为例.
<br>原始特征矩阵![](http://latex.codecogs.com/gif.latex?X_{p \times n}); 
转换矩阵![](http://latex.codecogs.com/gif.latex?W_{p \times d})；
线性变换后的新矩阵![](http://latex.codecogs.com/gif.latex?Y_{d \times n} = {W}'_{p \times d}\cdot X_{p \times n});
样本所属分类 label.
<br>其中,
<br>n为样本数,p为特征数,d为变换后的特征数.
<br>
<br>1. 使各类样本中心相隔距离尽可能远.
<br>变换后的样本中心:
<br>![](http://latex.codecogs.com/gif.latex?\widetilde{\mu_{j}} = \frac{1}{N_{j}}\sum_{i=1}^{n} I(y_{i}\in \omega _{j})y_{i}, \ \ \ j=0,1)
<br>其中,
<br>![](http://latex.codecogs.com/gif.latex?\omega _{j})指第j类；
![](http://latex.codecogs.com/gif.latex?N_{j})指第j类的样本数.
<br>
<br>因此,两类样本中心相隔距离为:
<br>![](http://latex.codecogs.com/gif.latex?|\widetilde{\mu_{0}}-\widetilde{\mu_{1}}|)
<br>这个值越大越好.
<br>
<br>2. 使各类样本的类内方差尽可能小.





<h4 id="1.4">1.4 局部线性嵌入 (LLE)</h4>

