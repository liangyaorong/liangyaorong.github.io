---
layout: post
title:  "常用优化方法及其原理"
date:   2017-04-03
---
<br>机器学习的各类算法中,常常需要用到最优化算法来求解参数.这篇文章就来总结一下机器学习中常用到的最优化算法.
<br>本文将会提到一下6种最优化算法,偏数理.
* 梯度下降法(gradient descent)
* 牛顿法(Newton's method)
* 拟牛顿法(Quasi-Newton Methods),
1. 包括BFGS
2. L-BFGS
3. DFP
4. Broyden类
5. OWL-QN

## 梯度下降法
<br>**梯度下降方法的导出**
<br>
<br>梯度下降法最核心的部分在于:"当f(x)在P0点可微时,f(x)在P0的梯度方向是f的值增长最快的方向,且沿该方向的变化率就是梯度的模."
<br>那么相应的,增长最快的方向就是f(x)在P0的负梯度方向了
<br>下面给出其证明:
1. 方向导数衡量函数在某特定方向l上的变化率,其定义为:
<br>![](http://latex.codecogs.com/gif.latex?f_{l}(P_{0})=\lim_{\rho \rightarrow 0}\frac{f(P)-f(P_{0})}{\rho })
<br>其中P为方向l上一点,rho为P与P0的距离

2. 可以证明(用一阶泰勒展开证),方向导数可表示为梯度与l的方向余弦的内积,即:
<br>![](http://latex.codecogs.com/gif.latex?\begin{align*}
f_{l}(P_{0})&=f_{x_{1}}(P_{0})cos(\alpha )+f_{x_{2}}(P_{0})cos(\beta )+f_{x_{3}}(P_{0})cos(\gamma  )+...\\
&=\triangledown f(P_{0})^{T}\cdot l_{0}\\
&=\left \| \triangledown f(P_{0}) \right \|\cdot \left \| l_{0} \right \|\cdot cos(\theta )
\end{align*}
)
<br>其中,限制l0为l方向上的单位向量,它的模为1; cos(theta)为梯度向量与l0的夹角
<br>这个其实挺容易理解,就是将l方向上的梯度分解到各坐标轴方向.要看详细证明的可以看<数学分析>"方向导数与梯度"章节.
<br>注意:要使这个式子成立,有前提: rho趋于0.因为求方向导数本来就是求 rho趋于0 时的极限.这也是后面讲到的迭代步长要小的原因


3. 可以看到,当theta为0,即l方向为梯度方向时,方向导数取最大值.且为梯度向量的模.
<br>因此,f在P0的梯度方向为f的值增长最快的方向.
<br>显然,这是在欧氏度量意义下才正确.但不在欧氏空间下的梯度一般没什么意义.

<br>下面用两个小例题解释一下什么叫"f(x)沿着梯度方向改变":

* 例一: ![](http://latex.codecogs.com/gif.latex?f(\overrightarrow{x})=x_{1}^{2}+x_{2}^{2}),求![](http://latex.codecogs.com/gif.latex?f(\overrightarrow{x}))在![](http://latex.codecogs.com/gif.latex?P_{0}(1,2))处的梯度.
<br>解: ![](http://latex.codecogs.com/gif.latex?\triangledown f(x)=(\frac{\partial f(x)}{\partial x_{1}},\frac{\partial f(x)}{\partial x_{2}})=(2x_{1},2x_{2}) \Rightarrow \triangledown f(P_{0})=(2,4))
<br>即P0处梯度方向是(2,4)
<br>
* 例二: P0为(1,2),从P0发出一条射线l,l方向为(2,4),求l方向上的点.
<br>解: 显然,答案为
<br>![](http://latex.codecogs.com/gif.latex?P_{0}+\lambda l=
\begin{bmatrix}
1\\ 
2
\end{bmatrix}
+\lambda \begin{bmatrix}
2\\ 
4
\end{bmatrix},\lambda >0)
<br>
<br>看到这应该已经非常直白了,f(x)沿着梯度方向改变,就是指其自变量x沿着梯度方向移动.
<br>注意移动的步长要足够小,因为![](http://latex.codecogs.com/gif.latex?\rho \rightarrow 0^{+})
<br>
<br>很自然推出梯度下降算法:
<br>![](http://img.blog.csdn.net/20170404112909737)
<br>
<br>**梯度下降法的收敛性**
<br>梯度下降法在"f(x)是连续可微实函数"的条件下是收敛的,且收敛于极值点
<br>严格的数学描述及证明看<最优化理论与算法 第二版> 陈宝林 P285
<br>
## 牛顿法
<br>上面讲的梯度下降法,我们提到它是用一阶泰勒展开来证明的
<br>我们看一下它一阶泰勒展开是什么样子:
<br>![](http://latex.codecogs.com/gif.latex?f(\vec{x})=f(\vec{x}^{(k)}+\triangledown f(\vec{x}^{(k)})^{T}(\vec{x}-\vec{x}^{(k)}))+o(\vec{x}-\vec{x}^{(k)}))
<br>那如果我们用二阶泰勒展开,又会得到什么呢?我们来看一下
<br>![](http://latex.codecogs.com/gif.latex?f(\vec{x})=f(\vec{x}^{(k)}+\triangledown f(\vec{x}^{(k)})^{T}(\vec{x}-\vec{x}^{(k)}))+\frac{1}{2}(\vec{x}-\vec{x}^{(k)})^{T}\triangledown ^{2}f(\vec{x}^{k})(\vec{x}-\vec{x}^{(k)})+o((\vec{x}-\vec{x}^{(k)})^{2}))
<br>泰勒展开的实质是用多项式去逼近原函数.
<br>对于梯度下降法,它只用了一阶泰勒展开,只有一次多项式的信息,因此要找到极值点只能通过方向导数,或者说斜率来寻找.
<br>而对于二阶泰勒展开,我们多二次多项式.
<br>二次多项式有个特点就是,它包含了极点的信息.由于该二次多项式是原函数的逼近,因此找到二次多项式的极值位置就能大概知道原函数的极值位置!
<br>这样的优化可不是星半点.我们可以想象,梯度下降就是往周围坡度最陡的方向一步一步走;而牛顿法是直接找到了谷底大概在哪,然后一下子就跳了过去.
<br>因此,牛顿法的收敛速度一般比梯度下降法快.
<br>直观的看就是这样:
<br>![](http://img.blog.csdn.net/20170404160329275)
<br>图中展示了sin(x)在x=0处的泰勒展开.从x=0开始迭代,实际极值点在C
<br>梯度下降法一次迭代后到达A;
<br>牛顿法一次迭代后到达B.
<br>显然牛顿法收敛更快.
<br>
<br>**牛顿法的导出**
<br>根据上面的分析,我们进行下面推导
1. 对f(x)进行泰勒展开,保留其二阶项,记为![](http://latex.codecogs.com/gif.latex?\phi (\vec{x})):
<br>![](http://latex.codecogs.com/gif.latex?\phi (\vec{x})=f(\vec{x}^{(k)}+\triangledown f(\vec{x}^{(k)})^{T}(\vec{x}-\vec{x}^{(k)}))+\frac{1}{2}(\vec{x}-\vec{x}^{(k)})^{T}\triangledown ^{2}f(\vec{x}^{k})(\vec{x}-\vec{x}^{(k)}))
2. 对![](http://latex.codecogs.com/gif.latex?\phi (\vec{x}))求偏导并令其为0:
<br>![](http://latex.codecogs.com/gif.latex?\triangledown \phi (x)=\triangledown f(x^{(k)})+\triangledown ^{2}f(x^{(k)})(x-x^{(k)})=0\\
\\
\Rightarrow x=x^{(k)}-\triangledown ^{2}f(x^{(k)})^{-1}\cdot \triangledown f(x^{(k)}))
<br>其中,
![](http://latex.codecogs.com/gif.latex?\triangledown ^{2}f(x^{(k)})=H(x^{(k)})\\
H(x^{(k)})_{i,j}=\begin{bmatrix}
\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}}
\end{bmatrix}_{n\times n})
<br>H(x)为海赛矩阵



