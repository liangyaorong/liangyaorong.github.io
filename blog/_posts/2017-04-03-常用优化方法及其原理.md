---
layout: post
title:  "常用优化方法及其原理"
date:   2017-04-03
---
<br>机器学习的各类算法中,常常需要用到最优化算法来求解参数.这篇文章就来总结一下机器学习中常用到的最优化算法.
<br>本文将会提到一下6种最优化算法,偏数理.
* 梯度下降法(gradient descent)
* 牛顿法(Newton's method)
* 拟牛顿法(Quasi-Newton Methods),
1. 包括BFGS
2. L-BFGS
3. DFP
4. Broyden类
5. OWL-QN

# 梯度下降法
<br>梯度下降法最核心的部分在于:"当f(x)在P0点可微时,f(x)在P0的梯度方向是f的值增长最快的方向,且沿该方向方向的变化率就是梯度的模."
<br>(摘自<数学分析>P135 第四版 下册 华东师大)
<br>那么相应的,增长最快的方向就是f(x)在P0的负梯度方向了
<br>下面给出其证明:
1. 方向导数衡量函数在某特定方向l上的变化率,其定义为:
<br>![](http://latex.codecogs.com/gif.latex?f_{l}(P_{0})=\lim_{\rho \rightarrow 0}\frac{f(P)-f(P_{0})}{\rho })
<br>其中P为方向l上一点
2. 可以证明,方向导数可表示为梯度与l的方向余弦的内积,即:
<br>![](http://latex.codecogs.com/gif.latex?\begin{align*}
f_{l}(P_{0})&=f_{x_{1}}(P_{0})cos(\alpha )+f_{x_{2}}(P_{0})cos(\beta )+f_{x_{3}}(P_{0})cos(\gamma  )+...\\
&=\triangledown f(P_{0})^{T}\cdot l_{0}\\
&=\left \| \triangledown f(P_{0}) \right \|\cdot \left \| l_{0} \right \|\cdot cos(\theta )
\end{align*}
)
<br>其中,l0为l方向上的单位向量,因此它的模为1; cos(theta)为梯度向量与l0的夹角
<br>这个其实挺容易理解,就是将l方向上的梯度分解到各坐标轴方向.要看详细证明的可以看<数学分析>"方向导数"章节
3. 可以看到,当theta为0,即l的方向为梯度方向时,方向导数取最大值.且为梯度向量的模.
<br>因此,f在P0的梯度方向为f的值增长最快的地方



对f(x)进行一阶泰勒展开:
<br>![](http://latex.codecogs.com/gif.latex?f(x^{k+1})=f(x^{k}+\Delta x)=f(x^{k})+f{}'(x^{k})\Delta x)

